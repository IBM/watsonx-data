{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32ddae07-079c-4165-a201-7debf6afdd07",
   "metadata": {},
   "source": [
    "\n",
    "# Build RAG with Haystack and watsonx.data Milvus\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for enhancing large language models with external knowledge sources. By combining the strengths of vector search and generative AI, RAG systems can provide more accurate, reliable, and contextually relevant responses.\n",
    "\n",
    "In this tutorial, we'll build a sophisticated RAG system using :\n",
    "\n",
    "- **Haystack**: An open-source framework for building production-ready LLM applications with modular components\n",
    "- **Milvus**: A high-performance vector database built specifically for embedding similarity search\n",
    "- **IBM watsonx.ai**: Enterprise-grade AI models with state-of-the-art capabilities for embedding and text generation\n",
    "\n",
    "By integrating these three technologies, we'll create a system that can:\n",
    "- Process and index documents\n",
    "- Generate high-quality vector embeddings\n",
    "- Store and efficiently retrieve similar documents\n",
    "- Generate contextually relevant answers to user queries\n",
    "  \n",
    "## What We'll Accomplish\n",
    "\n",
    "By the end of this tutorial, you'll have built a complete RAG system capable of:\n",
    "1. Indexing a text document into a vector database\n",
    "2. Converting user questions into semantic embeddings\n",
    "3. Retrieving relevant contextual information\n",
    "4. Generating accurate answers using retrieved context\n",
    "5. Responding to open-ended questions about the indexed data\n",
    "\n",
    "This combination offers several advantages for enterprise applications:\n",
    "\n",
    "1. **Data privacy**: Keep your data within your control by using IBM watsonx.ai instead of sending it to public APIs\n",
    "2. **Scalability**: Milvus provides production-grade vector search designed to handle millions of documents\n",
    "3. **Flexibility**: Haystack's component-based architecture allows easy customization and extension\n",
    "4. **Enterprise support**: IBM's watsonx.ai provides enterprise-grade models with proper support channels\n",
    "\n",
    "Let's dive in and build a complete RAG system with these technologies!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2c93f2-fce0-4218-90d2-11da357f5f92",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "### Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60d4d37b-152e-4a68-9322-79cc5739c94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.10 install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --quiet pymilvus milvus-haystack haystack-ai ibm-watsonx-ai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801b17a4-50c3-4c84-a97f-4491c38a9f93",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "\n",
    "First, we'll acquire a sample document about Leonardo da Vinci from Project Gutenberg. This will serve as our knowledge base for answering questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a928332-7e7d-4d61-be9f-2f677868fee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download sample data for demonstration\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "url = \"https://www.gutenberg.org/cache/epub/7785/pg7785.txt\"\n",
    "file_path = \"./davinci.txt\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    urllib.request.urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d2b9a5-d1a3-430b-b676-9e68a0c15656",
   "metadata": {},
   "source": [
    "## 3. IBM watsonx.ai Configuration\n",
    "\n",
    "Now, we'll set up the IBM watsonx.ai models that will power our RAG system. We'll need:\n",
    "- An embedding model to convert text into vector representations\n",
    "- A language model to generate human-like responses\n",
    "\n",
    "watsonx.ai provides powerful foundation models that are pre-trained on vast amounts of data. For our RAG system, we'll use:\n",
    "- **IBM Slate**: A powerful embedding model optimized for retrieval tasks\n",
    "- **IBM Granite**: A state-of-the-art language model for high-quality text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56bbfc8b-308c-43b6-ac96-ea5a6194b8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up IBM watsonx API credentials\n",
    "watsonx_credentials = {\n",
    "    \"url\": \"<watsonx url>\",  # Replace with your watsonx URL\n",
    "    \"apikey\":  \"<watsonx_api_key>\",  # Replace with your watsonx API Key\n",
    "}\n",
    "project_id = \"<project_id>\"  # Replace with your project ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2a709b8-d0a5-4ce5-8ea5-26ad31a2772c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import watsonx libraries\n",
    "from ibm_watsonx_ai import APIClient\n",
    "from ibm_watsonx_ai.foundation_models.embeddings import Embeddings\n",
    "from ibm_watsonx_ai.foundation_models import ModelInference\n",
    "from ibm_watsonx_ai.metanames import EmbedTextParamsMetaNames as EmbedParams\n",
    "\n",
    "# Initialize the IBM watsonx client\n",
    "client = APIClient(watsonx_credentials)\n",
    "\n",
    "# Configure embedding model\n",
    "embedding_model_id = \"ibm/slate-30m-english-rtrvr\"  \n",
    "embedding_params = {\n",
    "    EmbedParams.TRUNCATE_INPUT_TOKENS: 128,\n",
    "    EmbedParams.RETURN_OPTIONS: {'input_text': True},\n",
    "}\n",
    "\n",
    "# Initialize the embedding model\n",
    "watsonx_embeddings = Embeddings(\n",
    "    model_id=embedding_model_id,\n",
    "    credentials=watsonx_credentials,\n",
    "    params=embedding_params,\n",
    "    project_id=project_id,\n",
    "    space_id=None,\n",
    "    verify=False\n",
    ")\n",
    "\n",
    "# Configure LLM generation model\n",
    "generation_model_id = \"ibm/granite-3-3-8b-instruct\"  \n",
    "generation_params = {\n",
    "    \"max_new_tokens\": 1024,\n",
    "    \"temperature\": 0,  \n",
    "    \"top_p\": 0.9,\n",
    "    \"repetition_penalty\": 1.05\n",
    "}\n",
    "\n",
    "# Initialize the LLM model\n",
    "watsonx_llm = ModelInference(\n",
    "    model_id=generation_model_id,\n",
    "    credentials=watsonx_credentials,\n",
    "    params=generation_params,\n",
    "    project_id=project_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d7b093-17a3-4f68-a4f7-b680a84b2614",
   "metadata": {},
   "source": [
    "## 4. Haystack Integration\n",
    "\n",
    "Haystack provides a modular approach to building NLP pipelines through components that can be connected in various ways. To integrate IBM watsonx.ai models with Haystack, we need to create custom components that wrap around watsonx.ai's API calls.\n",
    "\n",
    "First, we'll create wrapper functions to simplify the interaction with watsonx.ai:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be087c2d-0820-4ec5-aecc-eba8022d94a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_documents(texts):\n",
    "    \"\"\"Wrapper function to embed documents using watsonx\"\"\"\n",
    "    return watsonx_embeddings.embed_documents(texts=texts)\n",
    "\n",
    "def embed_query(text):\n",
    "    \"\"\"Wrapper function to embed a single query using watsonx\"\"\"\n",
    "    return watsonx_embeddings.embed_query(text=text)\n",
    "\n",
    "def generate_text(prompt):\n",
    "    \"\"\"Wrapper function to generate text using watsonx LLM\"\"\"\n",
    "    response = watsonx_llm.generate(prompt=prompt)\n",
    "    return response['results'][0]['generated_text']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b341ce67-ad0d-4ae5-8b2f-971df90a8586",
   "metadata": {},
   "source": [
    "### Now we'll create custom Haystack components that use these wrapper functions. These components will slot into our pipelines, enabling us to use watsonx.ai within the Haystack framework:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b79a133-d524-47ec-839f-b54233e7703c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import component\n",
    "from haystack.dataclasses import Document\n",
    "from typing import Dict, List, Optional, Any\n",
    "\n",
    "# Create minimal custom components that use our wrapper functions\n",
    "# Haystack's @component decorator allows you to plug in your logic into pipelines.\n",
    "@component\n",
    "class watsonxDocumentEmbedder:\n",
    "    @component.output_types(documents=List[Document])\n",
    "    def run(self, documents: List[Document]):\n",
    "        texts = [doc.content for doc in documents]\n",
    "        embeddings = embed_documents(texts)\n",
    "        \n",
    "        for doc, embedding in zip(documents, embeddings):\n",
    "            doc.embedding = embedding\n",
    "            \n",
    "        return {\"documents\": documents}\n",
    "\n",
    "@component\n",
    "class watsonxTextEmbedder:\n",
    "    @component.output_types(embedding=List[float], text=str)\n",
    "    def run(self, text: str):\n",
    "        embedding = embed_query(text)\n",
    "        return {\"embedding\": embedding, \"text\": text}\n",
    "\n",
    "@component\n",
    "class watsonxGenerator:\n",
    "    @component.output_types(replies=List[str])\n",
    "    def run(self, prompt: str):\n",
    "        generated_text = generate_text(prompt)\n",
    "        return {\"replies\": [generated_text]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1d6c0e-9db0-45ca-ac73-c726d8608f02",
   "metadata": {},
   "source": [
    "### 5. Milvus Vector Database Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b55a93fd-b27a-4973-b81f-8c0f154b4b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Milvus document store initialized\n"
     ]
    }
   ],
   "source": [
    "# Import Haystack and Milvus components\n",
    "from haystack import Pipeline\n",
    "from haystack.components.converters import TextFileToDocument\n",
    "from haystack.components.preprocessors import DocumentSplitter\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.components.builders import PromptBuilder\n",
    "from milvus_haystack import MilvusDocumentStore\n",
    "from milvus_haystack.milvus_embedding_retriever import MilvusEmbeddingRetriever\n",
    "\n",
    "# Initialize Milvus document store\n",
    "document_store = MilvusDocumentStore(\n",
    "     connection_args={\n",
    "        \"uri\": \"https://<hostname>:<port>\", # Replace with your watsonx.data Milvus URI or IP\n",
    "        \"user\":\"<user>\",\n",
    "        \"password\":\"<password>\",\n",
    "        \"secure\": True,  # Set True if TLS is enabled\n",
    "        \"server_pem_path\": \"/root/path of ca.cert\"\n",
    "    }, \n",
    "    drop_old=True,\n",
    ")\n",
    "\n",
    "print(\"Milvus document store initialized\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a2d93c-1a86-4343-a945-4c460447eb9f",
   "metadata": {},
   "source": [
    "### 6. Indexing Pipeline\n",
    "\n",
    "Now that we have our components set up, we'll create an indexing pipeline to process our documents and store them in the Milvus vector database.\n",
    "\n",
    "This pipeline will:\n",
    "1. Load the text file\n",
    "2. Split it into smaller chunks for better retrieval\n",
    "3. Generate embeddings for each chunk using watsonx.ai\n",
    "4. Store both the text and embeddings in Milvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ce81bb8-c895-4b15-9d47-0df022eb5b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No abbreviations file found for en. Using default abbreviations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running indexing pipeline...\n",
      "Number of documents indexed: 191\n"
     ]
    }
   ],
   "source": [
    "# Create an indexing pipeline to process and store documents\n",
    "indexing_pipeline = Pipeline()\n",
    "indexing_pipeline.add_component(\"converter\", TextFileToDocument())\n",
    "indexing_pipeline.add_component(\n",
    "    \"splitter\", DocumentSplitter(split_by=\"sentence\", split_length=2)\n",
    ")\n",
    "indexing_pipeline.add_component(\"embedder\",watsonxDocumentEmbedder())\n",
    "indexing_pipeline.add_component(\"writer\", DocumentWriter(document_store))\n",
    "\n",
    "# Connect indexing pipeline components\n",
    "indexing_pipeline.connect(\"converter\", \"splitter\")\n",
    "indexing_pipeline.connect(\"splitter\", \"embedder\")\n",
    "indexing_pipeline.connect(\"embedder\", \"writer\")\n",
    "\n",
    "# Run the indexing pipeline\n",
    "print(\"Running indexing pipeline...\")\n",
    "indexing_pipeline.run({\"converter\": {\"sources\": [file_path]}})\n",
    "print(f\"Number of documents indexed: {document_store.count_documents()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b81004-d5ff-499e-bcee-0eb24eb33bf1",
   "metadata": {},
   "source": [
    "### 7. Testing Retrieval Capabilities\n",
    "\n",
    "Before building our complete RAG system, let's test the retrieval capabilities to ensure we can find relevant documents. We'll create a simple retrieval pipeline and test it with a question about the \"Warrior\" painting mentioned in our document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdecf0bb-c0b2-4e0b-aea8-5486a1c8df91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing retrieval with question: Where is the painting \"Warrior\" currently stored?\n",
      "\n",
      "Retrieved documents:\n",
      "===================\n",
      "Document 1:\n",
      "To about this period belongs the superb drawing of the \"Warrior,\" now\n",
      "in the Malcolm Collection in the British Museum. This drawing may have\n",
      "been made while Leonardo still frequented the studio of Andrea del\n",
      "Verrocchio, who in 1479 was commissioned to execute the equestrian\n",
      "statue of Bartolommeo Colleoni, which was completed twenty years later\n",
      "and still adorns the Campo di San Giovanni e Paolo in Venice.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Document 2:\n",
      "Some of these in red and black chalk are now preserved\n",
      "in the Royal Library at Windsor, where there are in all 145 drawings\n",
      "by Leonardo.\n",
      "\n",
      "Several other old copies of the fresco exist, notably the one in the\n",
      "Louvre. \n",
      "--------------------------------------------------\n",
      "Document 3:\n",
      "1252). As a matter of course it is\n",
      "unfinished, only the under-painting and the colouring of the figures\n",
      "in green on a brown ground having been executed. \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Define a test question\n",
    "question = 'Where is the painting \"Warrior\" currently stored?'\n",
    "\n",
    "# Create and run a simple retrieval pipeline\n",
    "retrieval_pipeline = Pipeline()\n",
    "retrieval_pipeline.add_component(\"embedder\", watsonxTextEmbedder())\n",
    "retrieval_pipeline.add_component(\n",
    "    \"retriever\", MilvusEmbeddingRetriever(document_store=document_store, top_k=3)\n",
    ")\n",
    "retrieval_pipeline.connect(\"embedder.embedding\", \"retriever.query_embedding\")\n",
    "\n",
    "print(\"\\nTesting retrieval with question:\", question)\n",
    "retrieval_results = retrieval_pipeline.run({\"embedder\": {\"text\": question}})\n",
    "\n",
    "# Display retrieved documents\n",
    "print(\"\\nRetrieved documents:\")\n",
    "print(\"===================\")\n",
    "for i, doc in enumerate(retrieval_results[\"retriever\"][\"documents\"], 1):\n",
    "    print(f\"Document {i}:\")\n",
    "    print(doc.content)\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7841bc24-b026-41a2-817a-83a229f63d0d",
   "metadata": {},
   "source": [
    "### 8. Building the Complete RAG Pipeline\n",
    "\n",
    "Now that we've confirmed our retrieval works, let's build the complete RAG pipeline. This pipeline will:\n",
    "1. Convert the user query into an embedding\n",
    "2. Retrieve relevant context from Milvus\n",
    "3. Create a prompt that includes the query and retrieved context\n",
    "4. Generate a response using the watsonx.ai language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af99c9e5-8973-4aa0-8b09-44bcbda5c6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PromptBuilder has 2 prompt variables, but `required_variables` is not set. By default, all prompt variables are treated as optional, which may lead to unintended behavior in multi-branch pipelines. To avoid unexpected execution, ensure that variables intended to be required are explicitly set in `required_variables`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running RAG pipeline...\n",
      "\n",
      "RAG Answer:\n",
      "==========\n",
      " The painting \"Warrior\" is currently stored in the Malcolm Collection in the British Museum.\n"
     ]
    }
   ],
   "source": [
    "# Define a prompt template for RAG\n",
    "prompt_template = \"\"\"\n",
    "Answer the following query based on the provided context. If the context does\n",
    "not include an answer, reply with 'I don't know'.\n",
    "\n",
    "Query: {{query}}\n",
    "\n",
    "Documents:\n",
    "{% for doc in documents %}\n",
    "{{ doc.content }}\n",
    "{% endfor %}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# Create the full RAG pipeline\n",
    "rag_pipeline = Pipeline()\n",
    "rag_pipeline.add_component(\"text_embedder\",watsonxTextEmbedder())\n",
    "rag_pipeline.add_component(\n",
    "    \"retriever\", MilvusEmbeddingRetriever(document_store=document_store, top_k=3)\n",
    ")\n",
    "rag_pipeline.add_component(\"prompt_builder\", PromptBuilder(template=prompt_template))\n",
    "rag_pipeline.add_component(\"generator\", watsonxGenerator())\n",
    "\n",
    "# Connect RAG pipeline components\n",
    "rag_pipeline.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n",
    "rag_pipeline.connect(\"retriever.documents\", \"prompt_builder.documents\")\n",
    "rag_pipeline.connect(\"prompt_builder\", \"generator\")\n",
    "\n",
    "# Run the RAG pipeline\n",
    "print(\"\\nRunning RAG pipeline...\")\n",
    "rag_results = rag_pipeline.run(\n",
    "    {\n",
    "        \"text_embedder\": {\"text\": question},\n",
    "        \"prompt_builder\": {\"query\": question},\n",
    "    }\n",
    ")\n",
    "\n",
    "# Display the final answer\n",
    "print(\"\\nRAG Answer:\")\n",
    "print(\"==========\")\n",
    "print(rag_results[\"generator\"][\"replies\"][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047df7df-afd5-4200-afbb-043fb190970a",
   "metadata": {},
   "source": [
    "### We can see the answer retrieved from our RAG Pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e4bc40-58a2-452b-a5d1-1b3ba8fa6cbb",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we've built a complete Retrieval-Augmented Generation (RAG) system by integrating three powerful technologies:\n",
    "\n",
    "1. **IBM watsonx.ai** provided the AI brains of our system with:\n",
    "   - The Slate embedding model to create semantic representations of text\n",
    "   - The Granite language model to generate natural language responses\n",
    "\n",
    "2. **Milvus** served as our vector database, enabling:\n",
    "   - Efficient storage of document embeddings\n",
    "   - Fast similarity search to find relevant context\n",
    "\n",
    "3. **Haystack** tied everything together with:\n",
    "   - Modular pipeline components\n",
    "   - Flexible document processing\n",
    "   - Seamless integration of different technologies\n",
    "\n",
    "This RAG system demonstrates how enterprises can leverage their private data to enhance AI capabilities. By retrieving relevant information and providing it as context to language models, we ensure more accurate, factual, and contextually appropriate responses.\n",
    "\n",
    "By combining the strengths of IBM watsonx.ai, Milvus, and Haystack, you now have a powerful, flexible RAG system that can be customized for a wide range of enterprise applications.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160d0c6c-eea1-4d8c-aacd-0b60069036e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
