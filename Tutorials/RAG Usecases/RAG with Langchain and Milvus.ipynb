{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7fab999-64c3-490b-bd99-29e0572e4465",
   "metadata": {},
   "source": [
    "# Building RAG with Langchain and watsonx.data Milvus\n",
    "\n",
    "In this notebook, we will explore the process of building a Retrieval-Augmented Generation (RAG) system using Langchain, Milvus, and watsonx.ai models. RAG is a powerful method that combines information retrieval with language generation, enabling systems to retrieve relevant documents and generate meaningful responses based on them.\n",
    "\n",
    "We will be using Milvus, to store and manage high-dimensional vector embeddings. These embeddings represent the knowledge contained in documents and are used for efficient similarity search. Then, we'll integrate Langchain, a framework designed for building applications with language models, to facilitate the retrieval and generation of responses from the stored data.\n",
    "\n",
    "Finally, we'll utilize watsonx.ai pre-trained models to enhance the system's ability to generate contextually rich, accurate, and relevant answers. This combination of cutting-edge technologies allows us to create an intelligent, scalable, and high-performing RAG system capable of delivering powerful insights from large data sets.\n",
    "\n",
    "## What Are We Trying to Achieve?\n",
    "Our goal is to create a retrieval-augmented generation (RAG) chain that:\n",
    "\n",
    "Retrieves relevant content from a set of documents.\n",
    "\n",
    "Embeds and indexes that content into a vector store using Milvus.\n",
    "\n",
    "Uses a powerful foundation model from watsonx.ai to answer questions based on retrieved context.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddd6e08-2d14-4b03-8a6d-7141a781e21c",
   "metadata": {},
   "source": [
    "## Prerequisites -\n",
    "Before proceeding, ensure that your environment is set up with the necessary libraries.These libraries are essential for loading, processing, and working with documents and embeddings. Follow the steps below to set up your environment:\n",
    "\n",
    "1. Install the necessary libraries.\n",
    "\n",
    "2. Ensure you have:\n",
    "-\tIBM watsonx.ai service instance (URL, API key, project ID)\n",
    "-\tIBM watsonx.data Milvus instance (Connection params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88941c92-e795-4ff9-92e4-0f73c41d3cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.10 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade --quiet  langchain langchain-community  langchain-milvus  langchain_ibm ibm-watson-machine-learning>=1.0.327 unstructured"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc1bbbd-4a20-48c7-812b-db88ad9f0cdc",
   "metadata": {},
   "source": [
    "## Authentication Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c30a01a9-a566-49c4-a80c-bcd41ae567a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai import APIClient\n",
    "\n",
    "# Set up WatsonX API credentials\n",
    "my_credentials = {\n",
    "    \"url\": \"<watsonx URL>\",  # Replace with your your service instance url (watsonx URL)\n",
    "    \"apikey\": '<watsonx_api_key>'  # Replace with your watsonx_api_key\n",
    "}\n",
    "\n",
    "\n",
    "# Initialize the WatsonX client for embeddings\n",
    "client = APIClient(my_credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f48309d-7490-47a1-bc7f-ee2200f45977",
   "metadata": {},
   "source": [
    "## Loading and Splitting Documents-\n",
    "We start by collecting content from IBM-related blog pages and announcements. For this, we use Langchain’s WebBaseLoader and split the text into manageable chunks using a recursive character splitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a58b768-4bc9-42da-bc4d-621000dcd2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Registration opened on the Think website on 21 January 2025. Or a direct link to register is available here.Yes, the Think® 2025 event in Boston will be a fee-based event. Standard pricing is USD 1,899.00, effective 21 January through 8 May 2025.Yes. All attendees must be at least 21 years of age by the day they pick up their conference badge.Requests for cancellation must be made in writing to IBMThink@gpj.com. Requests received by 4 April 2025, 11:59 PM ET will receive a full refund. Requests received after 12:00 AM ET, 5 April 2025 will not be eligible for a refund unless within 24-hours of registration.Substitutions for purchased passes will be allowed for attendees from the same company, and processed free of charge if requested by 11:59 PM ET on 4 April 2025. Substitution requests received after 12:00 AM ET, 5 April 2025 are subject to a USD 50 transaction fee.Substitutions for complimentary pass holders are not permitted. Please contact your IBM account representative or the' metadata={'source': 'https://www.ibm.com/events/think/faq'}\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Create a WebBaseLoader instance to load documents from web sources\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\n",
    "        \"https://www.ibm.com/events/think/faq\",\n",
    "        \"https://www.ibm.com/events/think/agenda\",\n",
    "        \"https://www.ibm.com/products/watsonx-ai\",\n",
    "        \"https://www.ibm.com/products/watsonx-ai/foundation-models\",\n",
    "        \"https://www.ibm.com/watsonx/pricing\",\n",
    "        \"https://www.ibm.com/watsonx\",\n",
    "        \"https://www.ibm.com/products/watsonx-data\",\n",
    "        \"https://www.ibm.com/products/watsonx-assistant\",\n",
    "        \"https://www.ibm.com/products/watsonx-code-assistant\",\n",
    "        \"https://www.ibm.com/products/watsonx-orchestrate\",\n",
    "        \"https://www.ibm.com/products/watsonx-governance\",\n",
    "        \"https://research.ibm.com/blog/granite-code-models-open-source\",\n",
    "        \"https://www.redhat.com/en/about/press-releases/red-hat-delivers-accessible-open-source-generative-ai-innovation-red-hat-enterprise-linux-ai\",\n",
    "        \"https://www.ibm.com/blog/announcement/enterprise-grade-model-choices/\",\n",
    "        \"https://www.ibm.com/blog/announcement/democratizing-large-language-model-development-with-instructlab-support-in-watsonx-ai/\",\n",
    "        \"https://newsroom.ibm.com/Blog-IBM-Consulting-Expands-Capabilities-to-Help-Enterprises-Scale-AI\",\n",
    "        \"https://www.ibm.com/products/data-product-hub\",\n",
    "        \"https://www.ibm.com/blog/announcement/delivering-superior-price-performance-and-enhanced-data-management-for-ai-with-ibm-watsonx-data/\",\n",
    "        \"https://www.ibm.com/blog/a-new-era-in-bi-overcoming-low-adoption-to-make-smart-decisions-accessible-for-all/\",\n",
    "        \"https://www.ibm.com/blog/announcement/ibm-watsonx-code-assistant-for-z-accelerate-the-application-lifecycle-with-generative-ai-and-automation/\",\n",
    "        \"https://www.ibm.com/blog/announcement/watsonx-code-assistant-java/\",\n",
    "        \"https://www.ibm.com/blog/announcement/watsonx-orchestrate-ai-z-assistant/\",\n",
    "        \"https://newsroom.ibm.com/Blog-How-IBM-Cloud-is-Accelerating-Business-Outcomes-with-Gen-AI\",\n",
    "        \"https://newsroom.ibm.com/2024-05-21-IBM-Unveils-Next-Chapter-of-watsonx-with-Open-Source,-Product-Ecosystem-Innovations-to-Drive-Enterprise-AI-at-Scale\",\n",
    "        \"https://www.ibm.com/products/concert\",\n",
    "        \"https://newsroom.ibm.com/2024-01-17-IBM-Introduces-IBM-Consulting-Advantage,-an-AI-Services-Platform-and-Library-of-Assistants-to-Empower-Consultants\",\n",
    "        \"https://www.ibm.com/consulting/info/ibm-consulting-advantage\"\n",
    "                \n",
    "    ),\n",
    "    bs_kwargs = dict(\n",
    "    parse_only=bs4.SoupStrainer(name=[\"main\", \"article\", \"p\"])\n",
    "    ),\n",
    ")\n",
    "# Load documents from web sources using the loader\n",
    "documents = loader.load()\n",
    "# Initialize a RecursiveCharacterTextSplitter for splitting text into chunks\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "# Split the documents into chunks using the text_splitter\n",
    "docs = splitter.split_documents(documents)\n",
    "print(docs[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f411cddd-f893-4a56-bae3-5ce6f91b37b5",
   "metadata": {},
   "source": [
    "## Embedding Configuration-\n",
    "IBM watsonx.ai offers several embedding models. Here we use SLATE_30M_ENGLISH_RTRVR, with truncation enabled. Each text chunk is converted into a vector (a list of numbers) using SLATE_30M_ENGLISH_RTRVR from IBM watsonx.ai.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da1aa127-a670-4732-a4d2-9b245ba6f77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai.foundation_models.embeddings import Embeddings\n",
    "from ibm_watsonx_ai.metanames import EmbedTextParamsMetaNames as EmbedParams\n",
    "\n",
    "model_id = client.foundation_models.EmbeddingModels.SLATE_30M_ENGLISH_RTRVR\n",
    "\n",
    "# Define embedding parameters\n",
    "embed_params = {\n",
    "    EmbedParams.TRUNCATE_INPUT_TOKENS: 128,  # Adjust token truncation as needed\n",
    "    EmbedParams.RETURN_OPTIONS: {'input_text': True},\n",
    "}\n",
    "\n",
    "# Set up the embedding model\n",
    "embedding = Embeddings(\n",
    "    model_id=model_id,\n",
    "    credentials=my_credentials,\n",
    "    params=embed_params,\n",
    "    project_id=\"<project_id>\",  # Replace with your project ID\n",
    "    space_id=None,\n",
    "    verify=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c400531-2b91-4c0e-8d2f-eaeda33e6fd7",
   "metadata": {},
   "source": [
    "## Generate Embeddings\n",
    "Each document chunk is converted into a high-dimensional vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ab819db-cc6c-48e2-9e33-722c1ee4ac25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming embedding.embed_documents() can process a list of text chunks\n",
    "embedding_vectors = embedding.embed_documents(texts=[doc.page_content for doc in docs])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1927f7c7-048e-4024-a81a-9a1f34455afb",
   "metadata": {},
   "source": [
    "## Store Embeddings in Milvus\n",
    "\n",
    "We will initialize a Milvus vector store with the documents, which load the documents into the Milvus vector store and build an index under the hood.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24e78c7d-45b5-4c36-bc50-ef536a8038a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connected\n"
     ]
    }
   ],
   "source": [
    "from langchain_milvus import Milvus\n",
    "\n",
    "vectorstore = Milvus.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=embedding,\n",
    "     connection_args={\n",
    "        \"uri\": \"https://<hostname>:<port>\", # Replace with your watsonx.data Milvus URI or IP\n",
    "        \"user\":\"<user>\",\n",
    "        \"password\":\"<password>\",\n",
    "        \"secure\": True,  # Set True if TLS is enabled\n",
    "        \"server_pem_path\": \"/path_to_ca.cert\"\n",
    "    }, \n",
    "    drop_old=True\n",
    ")\n",
    "\n",
    "\n",
    "print(\"connected\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741c4c01-55c6-4b0c-bd62-fa1d9fe1ec2f",
   "metadata": {},
   "source": [
    "## Perform a Search Query\n",
    "You may wonder: Why do we run a search query before using a language model?\n",
    "\n",
    "Because in a RAG pipeline, the retriever step brings in relevant knowledge before generation. So our language model doesn’t generate from thin air — it generates from facts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a753f2de-db97-4186-8eb6-4e2346c220e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'pk': 457518948738408506, 'source': 'https://www.ibm.com/products/watsonx-ai'}, page_content='trusted and cost-effective, including IBM Granite models, select open-source models from Hugging Face, third-party models from strategic partners and custom foundation models.DeepSeek R1 Distilled Models now available on watsonx.aiLearn more about DeepSeek-R1 on IBM watsonx.ai and how you can seamlessly integrate, fine-tune, and deploy with watsonx.ai’s secure, governed environment.\\xa0Whether you need code explanation, campaigns or lesson planning, use fit-for-purpose foundation models to get a head start on\\xa0quality content, both general and personalized.AI developers can build and deploy ready-to-use knowledge management applications quickly with pre-built RAG templates, frameworks and APIs.Streamline discovery and analysis of large amounts of data for faster, more valuable insights and forecasts specific to your needs and business requirements.From more customer satisfaction to deeper analysis, the numbers tell the storyAddAI saw 50% fewer unanswered customer service queriesSilver Egg')]\n"
     ]
    }
   ],
   "source": [
    "query = \"Describe in detail some of the foundational models in watsonx-ai?\"\n",
    "print(vectorstore.similarity_search(query, k=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74c22e3-a2e9-45fb-8f3c-7cf006a11199",
   "metadata": {},
   "source": [
    "## Set up watsonx.ai Language Model\n",
    "\n",
    "We use ibm/granite-3-3-8b-instruct for answering the user query:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92ad4381-7046-412f-9830-e84c2313bf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai.foundation_models import ModelInference\n",
    "from langchain_ibm import WatsonxLLM\n",
    "\n",
    "# Initialize model inference\n",
    "model_inference = ModelInference(\n",
    "    model_id=\"ibm/granite-3-3-8b-instruct\",  # Use a watsonx.ai foundational model\n",
    "     params={\n",
    "        \"max_new_tokens\": 1024         \n",
    "    },    \n",
    "    credentials=my_credentials,\n",
    "    project_id=\"<project_id>\"\n",
    ")\n",
    "\n",
    "# Wrap with LangChain's WatsonxLLM\n",
    "llm = WatsonxLLM(watsonx_model=model_inference)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1234c5a-92fc-4fa3-88db-c719e7a15b9b",
   "metadata": {},
   "source": [
    "## Compose the Final RAG Chain\n",
    "Let’s glue everything together using Langchain’s composable Runnable interface. Here’s how it works:\n",
    "\n",
    "* Retriever fetches top documents.\n",
    "\n",
    "* Formatter prepares them for prompt injection.\n",
    "\n",
    "* PromptTemplate frames a question and context.\n",
    "\n",
    "* LLM generates an answer.\n",
    "\n",
    "* OutputParser extracts the response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb29208b-0787-4718-9a66-cecdd1abb649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Watsonx-ai offers a variety of foundational models, including IBM Granite models, select open-source models from Hugging Face, third-party models from strategic partners, and custom foundation models. Among these, IBM Granite models are specifically highlighted for their performance, trustworthiness, and cost-effectiveness. These models are developed with a robust process that involves searching for and removing duplication, employing URL blocklists, filters for objectionable content, document quality checks, sentence splitting, and tokenization techniques before model training. Additionally, DeepSeek R1 Distilled Models are now available on watsonx-ai, which have been integrated into the InstructLab community. Developers can contribute to enhancing these models, similar to open-source projects, fostering a collaborative environment for model improvement. The platform also provides pre-built RAG (Retrieval-Augmented Generation) templates, frameworks, and APIs for AI developers to quickly build and deploy knowledge management applications. Furthermore, watsonx-ai offers a comprehensive data science toolset, enabling users to build AI/ML models automatically, generate synthetic data, and develop predictive and prescriptive models using Python Notebooks or Rstudio, or directly in their preferred IDE.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Define a better prompt template with clearer instructions\n",
    "PROMPT_TEMPLATE = \"\"\"Generate a summary of the context that answers the question. Explain the answer in multiple steps if possible. \n",
    "Answer style should match the context. Ideal Answer Length 5-12 sentences.\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Create a PromptTemplate instance with the defined template and input variables\n",
    "prompt = PromptTemplate(\n",
    "    template=PROMPT_TEMPLATE, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "# Convert the vector store to a retriever\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "response = rag_chain.invoke(query)\n",
    "print(\"Answer:\", response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57f7e51-6ff3-4a9c-875c-9da5c1899225",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "Through this step-by-step guide, we've built RAG using Langchain, Milvus, and watsonx.ai models.\n",
    "\n",
    "This architecture enables:\n",
    "\n",
    "- Accurate and grounded responses\n",
    "\n",
    "- Scalable vector search with Milvus\n",
    "\n",
    "- Seamless use of IBM’s foundation models for intelligent generation\n",
    "\n",
    "By combining the power of retrieval with generation, this RAG system can help enterprises unlock insights from massive knowledge bases, powering next-gen assistants, customer support bots, or internal knowledge tools.\n",
    "\n",
    "Whether you're exploring RAG for research or real-world deployment, this pipeline gives you a robust foundation to build on.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
