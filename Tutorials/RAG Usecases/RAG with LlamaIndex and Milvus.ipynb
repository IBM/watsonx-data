{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6424a603-aab3-4545-8a2d-b3f67df2375a",
   "metadata": {},
   "source": [
    "# Build RAG With LlamaIndex and watsonx.data Milvus\n",
    "\n",
    "In this notebook, we will explore the process of building a Retrieval-Augmented Generation (RAG) system using LlamaIndex, Milvus, and watsonx.ai models. RAG is a powerful method that combines information retrieval with language generation, enabling systems to retrieve relevant documents and generate meaningful responses based on them.\n",
    "\n",
    "We will be using Milvus, to store and manage high-dimensional vector embeddings. These embeddings represent the knowledge contained in documents and are used for efficient similarity search. Then, we'll integrate LlamaIndex, a framework that can connect large language models (LLMs) with external data sources\n",
    "\n",
    "Finally, we'll utilize watsonx.ai pre-trained models to enhance the system's ability to generate contextually rich, accurate, and relevant answers. This combination of cutting-edge technologies allows us to create an intelligent, scalable, and high-performing RAG system capable of delivering powerful insights from large data sets.\n",
    "\n",
    "## Understanding the RAG Architecture\n",
    "Before diving into the implementation, let's understand the basic flow of a RAG system:\n",
    "- Data Preparation: Raw data (documents, text, etc.) is collected and processed.\n",
    "-\tEmbedding Generation: The processed data is converted into vector embeddings using an embedding model.\n",
    "-\tVector Storage: These embeddings are stored in a vector database (in our case, Milvus).\n",
    "-\tQuery Processing: When a user asks a question, the query is also converted to an embedding.\n",
    "-\tSimilarity Search: The system searches for the most similar vectors to the query embedding.\n",
    "-\tContext Generation: The retrieved relevant information is used as context.\n",
    "-\tResponse Generation: An LLM uses the retrieved context to generate a comprehensive answer.\n",
    "This architecture ensures that the AI's responses are factually grounded in your data, reducing the likelihood of hallucinations or generating incorrect information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40962342-5d59-4162-9ddf-2b82df1bb30b",
   "metadata": {},
   "source": [
    "## Step-by-Step Implementation\n",
    "Let's walk through the process of building a RAG system using LlamaIndex, Milvus, and watsonx.ai models:\n",
    "1. Create Milvus Instance on watsonx.data\n",
    "You can refer to this Getting Started with IBM watsonx.data Milvus . \n",
    "2. Set up a Watson Machine Learning service instance and API key\n",
    "1.\tCreate a Watson Machine Learning service instance (you can choose the Lite plan, which is a free instance).\n",
    "2.\tGenerate an API Key in WML. Save this API key for use in this tutorial.\n",
    "Associate the WML service to the project you created in watsonx.ai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edca7cc-f431-4b90-a6f7-365be6573a35",
   "metadata": {},
   "source": [
    "## Installing Required Libraries\n",
    "Our implementation requires several Python libraries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59898a9c-da94-46d0-b773-abd95266dcef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.10 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.10 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.10 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.10 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.10 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.10 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU llama-index\n",
    "%pip install -qU llama-index-llms-ibm\n",
    "%pip install -qU llama-index-postprocessor-ibm\n",
    "%pip install -qU llama-index-embeddings-ibm\n",
    "%pip install -qU llama-index-vector-stores-milvus\n",
    "%pip install -qU pymilvus>=2.4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357f3f25-d72d-4bf0-a90c-71259d112103",
   "metadata": {},
   "source": [
    "## Environment Configuration\n",
    "Set up the environment variables with your watsonx.ai credentials:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24090cd2-562c-4ed2-a007-740ece9c487f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WATSONX_URL\"] = \"<WATSONX_URL>\"\n",
    "os.environ[\"WATSONX_APIKEY\"] = '<WATSONX_APIKEY>'\n",
    "os.environ[\"WATSONX_PROJECT_ID\"] = '<WATSONX_PROJECT_ID>'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f7aa29-45ab-42c2-a884-d78648ed04a9",
   "metadata": {},
   "source": [
    "## Preparing Sample Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73f03cfb-6365-4aed-835b-675a268fee56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-05-06 22:31:02--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 75042 (73K) [text/plain]\n",
      "Saving to: ‘data/paul_graham_essay.txt’\n",
      "\n",
      "data/paul_graham_es 100%[===================>]  73.28K  --.-KB/s    in 0.02s   \n",
      "\n",
      "2025-05-06 22:31:02 (3.57 MB/s) - ‘data/paul_graham_essay.txt’ saved [75042/75042]\n",
      "\n",
      "--2025-05-06 22:31:03--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/10k/uber_2021.pdf\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1880483 (1.8M) [application/octet-stream]\n",
      "Saving to: ‘data/uber_2021.pdf’\n",
      "\n",
      "data/uber_2021.pdf  100%[===================>]   1.79M  --.-KB/s    in 0.09s   \n",
      "\n",
      "2025-05-06 22:31:03 (20.9 MB/s) - ‘data/uber_2021.pdf’ saved [1880483/1880483]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p 'data/'\n",
    "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham_essay.txt'\n",
    "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/10k/uber_2021.pdf' -O 'data/uber_2021.pdf'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8cfddc-b622-49e2-b6f9-396328c7db52",
   "metadata": {},
   "source": [
    "## Generate our data\n",
    "As a first example, lets generate a document from the file paul_graham_essay.txt. It is a single essay from Paul Graham titled What I Worked On. To generate the documents, we will use the SimpleDirectoryReader.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c6474b9-c158-4c1d-a6a7-853ccf335741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ID: a22be147-250b-41ef-8b28-ccf89bf8f86c\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# Set chunk size for document splitting\n",
    "Settings.chunk_size = 512\n",
    "\n",
    "# Load documents from file\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\"./data/paul_graham_essay.txt\"]\n",
    ").load_data()\n",
    "\n",
    "print(f\"Document ID: {documents[0].doc_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d26d708-b820-41a9-a916-03ea968a5cb8",
   "metadata": {},
   "source": [
    "## IBM watsonx.ai Configuration\n",
    "Next, we'll configure our connection to IBM watsonx.ai:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9cb0df7-1acc-42e3-b9ab-5d8631a6d5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai import APIClient\n",
    "\n",
    "# Set up WatsonX API credentials\n",
    "my_credentials = {\n",
    "    \"url\":  \"<watsonx_url>\",  # Replace with your your service instance url (watsonx URL)\n",
    "    \"apikey\": '<watsonx_api_key>' # Replace with your watsonx_api_key\n",
    "}\n",
    "\n",
    "\n",
    "# Initialize the WatsonX client for embeddings\n",
    "client = APIClient(my_credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d4cb19-efc0-4631-91b4-97dbab0d5fcf",
   "metadata": {},
   "source": [
    "## Initializing the Embedding Model\n",
    "We'll use IBM's slate-30m-english-rtrvr model for generating embeddings:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b00c2aa4-b738-40a1-a6cb-9f2284cd67a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.ibm import WatsonxEmbeddings\n",
    "\n",
    "# Truncating inputs to fit embedding model's context window\n",
    "truncate_input_tokens = 512\n",
    "\n",
    "# Initialize watsonx embedding model\n",
    "watsonx_embedding = WatsonxEmbeddings(\n",
    "    model_id=\"ibm/slate-30m-english-rtrvr\",  # Or any preferred embedding model\n",
    "    credentials=my_credentials,\n",
    "    project_id=\"<project_id>\",\n",
    "    truncate_input_tokens=truncate_input_tokens,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86295580-3a3c-46ee-b911-9b8e6457db6a",
   "metadata": {},
   "source": [
    "## Initializing the Language Model\n",
    "For text generation, we'll use Llama 3:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71cca19d-7c41-4da0-90ce-cb31d8456ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ibm import WatsonxLLM\n",
    "\n",
    "# Maximum tokens to generate in response\n",
    "max_new_tokens = 256\n",
    "\n",
    "# Initialize watsonx LLM\n",
    "watsonx_llm = WatsonxLLM(\n",
    "    model_id=\"meta-llama/llama-3-3-70b-instruct\",  # Or any preferred foundation model\n",
    "    credentials=my_credentials,\n",
    "    project_id=\"<project_id>\",\n",
    "    max_new_tokens=max_new_tokens,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d44a1f6-eaa3-47d7-bc7c-59f3dc9ebccc",
   "metadata": {},
   "source": [
    "## Setting Up Milvus Vector Store\n",
    "Now we configure LlamaIndex to use our Milvus instance:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a1220c4-5ebc-4064-aaf8-ca897e3019fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.milvus import MilvusVectorStore\n",
    "from llama_index.core import StorageContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e924cc7-5da2-4ffa-a871-41274302b09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 22:31:11,078 [DEBUG][_create_connection]: Created new connection using: bbdd600defec4476b2644f08d49d3340 (async_milvus_client.py:599)\n"
     ]
    }
   ],
   "source": [
    "vector_store = MilvusVectorStore(\n",
    "                         uri=\"https://<hostname>:<port>\",\n",
    "                         token=\"<user>:<password>\",\n",
    "                         server_pem_path=\"/root/path to ca.cert\",\n",
    "                         dim=384,\n",
    "                         overwrite=True,# refer Managing Vectors Collections Section\n",
    "                         collection_name=\"watsonx_llamaindex\",\n",
    " )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c06c393-0120-4bf6-aee1-be746521308d",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f23babb-5224-475e-a9c9-cc7ac942835a",
   "metadata": {},
   "source": [
    "## Creating the Index\n",
    "With all components in place, we create the vector index:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc5b5cd8-5603-4085-8fe9-32474b13f2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create index with watsonx embeddings and Milvus vector store\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents=documents, \n",
    "    embed_model=watsonx_embedding,\n",
    "    storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6faa546e-0fd1-42de-946d-41197d42a6d3",
   "metadata": {},
   "source": [
    "## Building a Query Engine\n",
    "The query engine retrieves the most relevant document chunks and generates a coherent response using the LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "262b5f02-14d5-4bcb-b52b-3e11168987d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Response:\n",
      "Final Response: In this essay, Sam Altman was asked to be the\n",
      "president of Y Combinator (YC) and initially said no because he wanted\n",
      "to start a startup to make nuclear reactors. However, he eventually\n",
      "agreed to take over as president, starting with the winter 2014 batch,\n",
      "and was given the freedom to reorganize YC. He learned the job and\n",
      "took over running YC, allowing the original founders, including Paul\n",
      "Graham, to retire or become ordinary partners.\n",
      "______________________________________________________________________\n",
      "Source Node 1/3\n",
      "Node ID: 8728b685-c0b1-4cf4-9c9b-038ef0a60b8f\n",
      "Similarity: 0.6335484385490417\n",
      "Text: [17]  As well as HN, I wrote all of YC's internal software in\n",
      "Arc. But while I continued to work a good deal in Arc, I gradually\n",
      "stopped working on Arc, partly because I didn't have time to, and\n",
      "partly because it was a lot less attractive to mess around with the\n",
      "language now that we had all this infrastructure depending on it. So\n",
      "now my three pr...\n",
      "______________________________________________________________________\n",
      "Source Node 2/3\n",
      "Node ID: bd8b11d8-d463-4a85-8fe5-53f688c61066\n",
      "Similarity: 0.6334127187728882\n",
      "Text: What I Worked On  February 2021  Before college the two main\n",
      "things I worked on, outside of school, were writing and programming. I\n",
      "didn't write essays. I wrote what beginning writers were supposed to\n",
      "write then, and probably still are: short stories. My stories were\n",
      "awful. They had hardly any plot, just characters with strong feelings,\n",
      "which I ...\n",
      "______________________________________________________________________\n",
      "Source Node 3/3\n",
      "Node ID: 81e9d37b-2f27-4a79-8341-dd55617d6be1\n",
      "Similarity: 0.6330976486206055\n",
      "Text: And it wasn't, so I would.  In the summer of 2012 my mother had\n",
      "a stroke, and the cause turned out to be a blood clot caused by colon\n",
      "cancer. The stroke destroyed her balance, and she was put in a nursing\n",
      "home, but she really wanted to get out of it and back to her house,\n",
      "and my sister and I were determined to help her do it. I used to fly\n",
      "up to...\n"
     ]
    }
   ],
   "source": [
    "# Create a query engine \n",
    "query_engine = index.as_query_engine(\n",
    "    llm=watsonx_llm,\n",
    "    similarity_top_k=3,  # Retrieve top 3 most similar nodes\n",
    ")\n",
    "\n",
    "# Execute the same query\n",
    "response_simple = query_engine.query(\n",
    "    \"What did Sam Altman do in this essay?\",\n",
    ")\n",
    "\n",
    "# Print the response with sources\n",
    "from llama_index.core.response.pprint_utils import pprint_response\n",
    "print(\"\\n\\nResponse:\")\n",
    "pprint_response(response_simple, show_source=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa757be6-b69f-43ac-b934-a0a04e1c91d6",
   "metadata": {},
   "source": [
    "**This is a Retrieval-Augmented Generation (RAG) use case. The query engine retrieved the top 3 most relevant text chunks and used them to answer the question. It summarized that Sam Altman initially declined the YC president role to build nuclear reactors but eventually accepted and restructured the organization. The sources show the retrieved content with similarity scores, although only one directly relates to the answer.**\n",
    "\n",
    "## Now, let’s check out a few more things."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3442e9b-145e-4c43-b8b6-98b14d6efa59",
   "metadata": {},
   "source": [
    "## Managing Vector Collections\n",
    "LlamaIndex and Milvus offer flexibility in how you manage your vector collections:\n",
    "### Overwriting Existing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99347b93-17e7-4069-8ecb-d6f4fdc4603c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 22:31:20,795 [DEBUG][_create_connection]: Created new connection using: 98caa97df1c04ed1af987d9f51ee603a (async_milvus_client.py:599)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:  There is no information about an author in the context. The context only mentions a number being searched for, which is ten. Therefore, it is not possible to determine the author based on the provided context.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Document\n",
    "#overwrite=True ( overwriting removes the previous data)\n",
    "\n",
    "vector_store = MilvusVectorStore(\n",
    "    uri=\"https://<hostname>:<port>\",\n",
    "    token=\"<user>:<password>\",\n",
    "    server_pem_path=\"/root/path to ca.cert\",\n",
    "    dim=384,\n",
    "    overwrite=True,\n",
    "    collection_name=\"watsonx_llamaindex\",)\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "# Create a new document\n",
    "new_doc = Document(text=\"The number that is being searched for is ten.\")\n",
    "\n",
    "# Create index with the new document and watsonx embedding model\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    [new_doc],\n",
    "    embed_model=watsonx_embedding,  # Use the watsonx embedding model we defined earlier\n",
    "    storage_context=storage_context,\n",
    ")\n",
    "\n",
    "# Try a more specific query\n",
    "res = query_engine.query(\"Who is the author?\")\n",
    "print(f\"Response: {res}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cbd229-87ec-4f92-b7e6-04a7c7f2972f",
   "metadata": {},
   "source": [
    "### Appending to Existing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1dc12298-e473-406d-84e1-7ee653b46bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 22:31:25,945 [DEBUG][_create_connection]: Created new connection using: 19783b8c21ed445ca896450d2aa5db93 (async_milvus_client.py:599)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: 10.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Document\n",
    "#overwrite=False (  adding additional data to an already existing index)\n",
    "\n",
    "vector_store = MilvusVectorStore(\n",
    "    uri=\"https://<hostname>:<port>\",\n",
    "    token=\"<user>:<password>\",\n",
    "    server_pem_path=\"/root/path to ca.cert\",\n",
    "    dim=384,\n",
    "    overwrite=False,\n",
    "    collection_name=\"watsonx_llamaindex\",)\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents=documents, \n",
    "    embed_model=watsonx_embedding,\n",
    "    storage_context=storage_context\n",
    ")\n",
    "\n",
    "# Try a more specific query\n",
    "res = query_engine.query(\"What is the number?\")\n",
    "print(f\"Response: {res}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99aae0f7-ef0e-4041-a458-d26d66d09739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Paul Graham.\n"
     ]
    }
   ],
   "source": [
    "res = query_engine.query(\"Who is the author?\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df1307e-8bdf-44fa-a9c5-54ef9f9cfd71",
   "metadata": {},
   "source": [
    "## Metadata filtering\n",
    "We can generate results by filtering specific sources. The following example illustrates loading all documents from the directory and subsequently filtering them based on metadata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89ebd7f0-f546-487d-ac20-2dc8f6d34d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 22:31:50,768 [DEBUG][_create_connection]: Created new connection using: b0163873bcce48ae9c9558e6a0fee9a5 (async_milvus_client.py:599)\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.vector_stores import ExactMatchFilter, MetadataFilters\n",
    "\n",
    "# Load all the two documents loaded before\n",
    "documents_all = SimpleDirectoryReader(\"./data/\").load_data()\n",
    "\n",
    "vector_store = MilvusVectorStore(\n",
    "    uri=\"https://<hostname>:<port>\",\n",
    "    token=\"<user>:<password>\",\n",
    "    server_pem_path=\"/root/path to ca.cert\",\n",
    "    dim=384,\n",
    "    overwrite=True,\n",
    "    collection_name=\"watsonx_llamaindex\",)\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "index = VectorStoreIndex.from_documents( documents=documents_all, \n",
    "                                         embed_model=watsonx_embedding,\n",
    "                                         storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346c1fd8-7bd3-4591-9cdf-08cc214efd5e",
   "metadata": {},
   "source": [
    "### We want to only retrieve documents from the file uber_2021.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f03409b-cf44-4388-a2c4-195c0fa3e460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The author faced difficulties such as reduced demand for Mobility offerings, accelerated growth of Delivery offerings, and challenges in managing driver availability and consumer demand due to the COVID-19 pandemic. The author also had to implement measures such as suspending shared rides, implementing \"leave at door\" delivery options, and asking employees to work remotely to comply with social distancing guidelines. Additionally, the author had to increase investments in driver incentives to improve driver availability. The author was also unable to accurately predict the full impact of COVID-19 on their business due to numerous uncertainties.\n"
     ]
    }
   ],
   "source": [
    "filters = MetadataFilters(\n",
    "    filters=[ExactMatchFilter(key=\"file_name\", value=\"uber_2021.pdf\")]\n",
    ")\n",
    "query_engine = index.as_query_engine( llm=watsonx_llm,\n",
    "                                      similarity_top_k=3,  # Retrieve top 3 most similar nodes,\n",
    "                                      filters=filters)\n",
    "res = query_engine.query(\"What difficulties did the author face due to the disease?\")\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a974587-608d-41b8-b447-1044fd6450c7",
   "metadata": {},
   "source": [
    "### We get a different result this time when retrieve from the file paul_graham_essay.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9dfcd10a-a8d9-4f97-8a00-1f74dae21e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " None. The text does not mention the author facing any difficulties due to a disease. It mentions the author facing stress due to Hacker News (HN), but this is not related to a disease. It also mentions a blister from an ill-fitting shoe as a metaphor for the stress caused by HN, but this is not a real disease or health issue.\n"
     ]
    }
   ],
   "source": [
    "filters = MetadataFilters(\n",
    "    filters=[ExactMatchFilter(key=\"file_name\", value=\"paul_graham_essay.txt\")]\n",
    ")\n",
    "query_engine = index.as_query_engine(llm=watsonx_llm,\n",
    "                                     similarity_top_k=3,  # Retrieve top 3 most similar nodes,\n",
    "                                     filters=filters)\n",
    "res = query_engine.query(\"What difficulties did the author face due to the disease?\")\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b006d6-7dcc-41f5-8b5b-1b6ab6c52b2c",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Building a RAG system with LlamaIndex, Milvus, and watsonx.ai models provides an elegant solution for creating knowledge-rich AI applications. This architecture separates concerns effectively:\n",
    "- LlamaIndex handles document processing and query orchestration\n",
    "-\tMilvus efficiently stores and retrieves vector embeddings\n",
    "-\twatsonx.ai provides powerful models for embedding generation and text generation\n",
    "\n",
    "This separation makes the system modular and maintainable, allowing you to swap components as needed or scale individual parts of the system.\n",
    "By following the steps outlined in this blog post, you can create a RAG system that provides accurate, contextually relevant responses grounded in your own data. Whether you're building a customer support chatbot, a document analysis tool, or a research assistant, the LlamaIndex-Milvus-watsonx.ai stack offers a robust foundation for your AI application.\n",
    "As LLM technology continues to evolve, the RAG architecture will remain relevant because it addresses one of the fundamental challenges of AI systems: connecting models to real-world, up-to-date information. By mastering RAG, you're preparing for the future of AI application development.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
