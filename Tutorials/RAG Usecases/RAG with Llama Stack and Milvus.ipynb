{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e40c71d-641c-4007-9bde-fbec825ee8b9",
   "metadata": {},
   "source": [
    "# Build RAG with Llama Stack with watsonx.data Milvus\n",
    "\n",
    "The Llama Stack is a set of open-source tools that work together to build powerful AI applications, especially LLM (Large Language Model) apps like chatbots, document search, and question answering systems.\n",
    "\n",
    "Llama Stack offers flexibility in how it's deployed—whether as a library, a standalone server, or a custom-built distribution. You can mix and match components with different providers, so the setup can vary widely based on your goals.\n",
    "\n",
    "In this tutorial, we’ll show you how to set up a Llama Stack Server with Milvus using watsonx.ai models. This setup will let you upload your own data and use it as your knowledge base. Then, we’ll run some example questions, creating a full RAG (Retrieval-Augmented Generation) app that can give helpful answers using your data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5b9b54-ab25-4f13-b5a1-c3f1393a37e7",
   "metadata": {},
   "source": [
    "## Setting Up the Environment\n",
    "\n",
    "### 1. Create Milvus Instance on watsonx.data\n",
    "\n",
    "You can refer to the [Getting Started guide with IBM watsonx.data Milvus](https://community.ibm.com/community/user/blogs/swati-karot/2025/02/06/getting-started-with-watsonxdata-milvus).\n",
    "\n",
    "### 2. Set up a Watson Machine Learning service instance and API key\n",
    "\n",
    "1. Create a [Watson Machine Learning](https://cloud.ibm.com/catalog/services/watson-machine-learning?utm_source=ibm_developer&utm_content=in_content_link&utm_id=tutorials_awb-create-langchain-rag-system-python-watsonx&cm_sp=ibmdev-_-developer-tutorials-_-trial) service instance (Lite plan is available).\n",
    "2. Generate and save an API Key for use in this tutorial.\n",
    "3. Associate the WML service to your project in watsonx.ai.\n",
    "\n",
    "### 3. Starting the Llama Stack Server\n",
    "\n",
    "#### Clone the Llama Stack Repo\n",
    "```bash\n",
    "git clone https://github.com/meta-llama/llama-stack.git\n",
    "cd llama-stack\n",
    "```\n",
    "\n",
    "#### Set Up a Conda Environment\n",
    "```bash\n",
    "conda create -n stack python=3.10 -y\n",
    "conda activate stack\n",
    "pip install -e .\n",
    "```\n",
    "\n",
    "####  Set Environment Variables\n",
    "Llama Stack will need environment variables to authenticate and configure services. Here, we are using the watsonx inference model. Set the following environment variables with your watsonx API key and project ID:\n",
    "```bash\n",
    "export WATSONX_API_KEY=\"<WATSONX_API_KEY>\"\n",
    "export WATSONX_PROJECT_ID=\"<WATSONX_PROJECT_ID>\"\n",
    "```\n",
    "Make sure you replace <WATSONX_API_KEY> and <WATSONX_PROJECT_ID> with your actual API key and project ID.\n",
    "\n",
    "\n",
    "#### Configure Milvus as Your Vector Store\n",
    "\n",
    "Edit the file: `llama_stack/templates/watsonx/run.yaml`\n",
    "\n",
    "Replace the `vector_io` section with:\n",
    "\n",
    "```yaml\n",
    "vector_io:\n",
    "- provider_id: milvus\n",
    "  provider_type: remote::milvus\n",
    "  config:\n",
    "    uri: http://localhost:19530\n",
    "    token: <user>:<Password>\n",
    "    secure: True\n",
    "    server_pem_path: \"path/to/server.pem\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7630bd-a36d-4e2e-9ac5-d7ad7014fcd8",
   "metadata": {},
   "source": [
    "## Building a Custom Distribution Using a Template\n",
    "\n",
    "### 1. Build the Distribution\n",
    "```bash\n",
    "llama stack build --template watsonx --image-type conda\n",
    "```\n",
    "\n",
    "### 2. Launch the Llama Stack Server\n",
    "```bash\n",
    "llama stack run --image-type conda ~/.llama/distributions/watsonx/watsonx-run.yaml\n",
    "```\n",
    "\n",
    "If everything goes well, you should see the Llama Stack server running on port 8321.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568cd10a-afde-4344-ba39-c13f873b309a",
   "metadata": {},
   "source": [
    "## Running RAG from the Client\n",
    "Once the Llama Stack server is up and running, the next step is to interact with it using client code. The script below demonstrates how to perform Retrieval-Augmented Generation (RAG) using your own documents.\n",
    "\n",
    "**Note**:This script must be executed inside the Llama Stack environment, such as within the Docker container or the Conda environment created by Llama Stack. This ensures access to the required dependencies, file paths, and the running Llama Stack service.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb592cdb-279a-434f-9102-5785ca8de850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import uuid  \n",
    "from llama_stack_client.types import Document  \n",
    "from llama_stack_client.lib.agents.agent import Agent  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b3a8f0c-0973-420e-9eb5-88f7a0de0b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the inference model and the port where LlamaStack is running\n",
    "INFERENCE_MODEL = \"meta-llama/llama-3-3-70b-instruct\"\n",
    "LLAMA_STACK_PORT = 8321\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c059046-efb7-4b1a-8561-2868815901d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a client for connecting to the local LlamaStack server\n",
    "def create_http_client():\n",
    "    from llama_stack_client import LlamaStackClient\n",
    "    return LlamaStackClient(base_url=f\"http://localhost:{LLAMA_STACK_PORT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f0ea2ec-3a3d-4030-84fb-24a19233e238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a client instance\n",
    "client = create_http_client()\n",
    "\n",
    "# List of file paths containing content to be inserted into Milvus\n",
    "doc_paths = [\n",
    "    \"/root/VP/milvus_intro.txt\",\n",
    "    \"/root/VP/collection.txt\",\n",
    "    \"/root/VP/schema.txt\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36538491-a32a-4521-a520-7fe012c91590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and convert the documents into Document objects with metadata\n",
    "documents = []\n",
    "for i, path in enumerate(doc_paths):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        documents.append(Document(\n",
    "            document_id=f\"milvus-doc-{i}\",  \n",
    "            content=content,  \n",
    "            mime_type=\"text/plain\",  \n",
    "            metadata={\"source\": path}  \n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84f692a1-ad27-4aa6-8aac-0baf59fd146e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorDBRegisterResponse(embedding_dimension=384, embedding_model='all-MiniLM-L6-v2', identifier='milvus-vector-db-1a92a8d20ee2467494a40b71507a9aa9', provider_id='milvus', provider_resource_id='milvus-vector-db-1a92a8d20ee2467494a40b71507a9aa9', type='vector_db', access_attributes=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a unique ID for the vector database using UUID\n",
    "vector_db_id = f\"milvus-vector-db-{uuid.uuid4().hex}\"\n",
    "\n",
    "# Register a new vector database in LlamaStack, using Milvus as the backend\n",
    "client.vector_dbs.register(\n",
    "    vector_db_id=vector_db_id,\n",
    "    embedding_model=\"all-MiniLM-L6-v2\",  # Model used to generate embeddings\n",
    "    embedding_dimension=384,  \n",
    "    provider_id=\"milvus\"  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "436bbe84-a721-494a-afd9-82224641c8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting Milvus docs into vector DB...\n"
     ]
    }
   ],
   "source": [
    "print(\"Inserting Milvus docs into vector DB...\")\n",
    "\n",
    "# Insert the documents into the vector DB using LlamaStack's built-in RAG tool\n",
    "client.tool_runtime.rag_tool.insert(\n",
    "    documents=documents,\n",
    "    vector_db_id=vector_db_id,\n",
    "    chunk_size_in_tokens=1024 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d0f4f14-a78a-4f19-a6e2-99db8fc6e0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a RAG agent using the selected LLM and the registered Milvus vector store\n",
    "rag_agent = Agent(\n",
    "    client=client,\n",
    "    model=INFERENCE_MODEL,  # LLM to use for generating responses\n",
    "    instructions=\"You are a Milvus expert assistant.\",  # System prompt to guide behavior\n",
    "    enable_session_persistence=False,  # Don't persist chat history\n",
    "    tools=[{\n",
    "        \"name\": \"builtin::rag\",  # Use the built-in RAG tool\n",
    "        \"args\": {\"vector_db_ids\": [vector_db_id]}  # Connect RAG to the created vector DB\n",
    "    }],\n",
    "    sampling_params={\n",
    "        \"max_tokens\": 2048,  # Max tokens for response generation\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d0bc4ae-48bf-4fd0-abc5-15db511aa3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a new chat session with the agent\n",
    "session_id = rag_agent.create_session(session_name=\"milvus-session\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52e19baa-bc4a-4d5a-8847-ecdbb71b8875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide a user question for the agent to answer\n",
    "user_prompt = \"What is Milvus ? Give it in bullet points\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38ed4822-948d-4721-a52a-64a190862817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from Milvus Bot:\n",
      "* Milvus is an open-source vector database built to power embedding similarity search and AI applications.\n",
      "* It is designed to manage large-scale vector data, such as embeddings generated by machine learning models.\n",
      "* Milvus provides a scalable and efficient way to store, index, and search vector data, enabling fast and accurate similarity searches.\n",
      "* It supports a wide range of indexing algorithms and distance metrics, allowing users to choose the best approach for their specific use case.\n",
      "* Milvus is often used in applications such as image and video search, natural language processing, recommendation systems, and more.\n",
      "* It provides a simple and intuitive API, making it easy to integrate with existing machine learning workflows and applications.\n",
      "* Milvus is highly scalable and can handle large volumes of data, making it suitable for large-scale AI applications.\n",
      "* It also provides features such as data partitioning, replication, and backup, ensuring high availability and reliability.\n",
      "* Milvus supports multiple data formats, including vectors, strings, and integers, and can be used with a variety of programming languages, including Python, Java, and C++.\n"
     ]
    }
   ],
   "source": [
    "# Generate a response to the user prompt within the session\n",
    "response = rag_agent.create_turn(\n",
    "    messages=[{\"role\": \"user\", \"content\": user_prompt}],\n",
    "    session_id=session_id,\n",
    "    stream=False  # Set to True to stream response if supported\n",
    ")\n",
    "\n",
    "# Print the AI assistant's response\n",
    "print(\"Response from Milvus Bot:\")\n",
    "print(response.output_message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993f83ec-6cd8-4114-9513-625a1c4d636e",
   "metadata": {},
   "source": [
    "# Understanding the Code\n",
    "\n",
    "1. **Client Setup**: Establishes a connection to the Llama Stack server  \n",
    "2. **Document Preparation**: Reads files and converts to Document objects  \n",
    "3. **Vector DB Registration**: Creates vector DB in Milvus  \n",
    "4. **Document Ingestion**: Uploads docs to Milvus DB  \n",
    "5. **RAG Agent Creation**: Initializes an agent  \n",
    "6. **Query Execution**: Sends a user query and retrieves the response  \n",
    "\n",
    "# Conclusion\n",
    "\n",
    "The integration of Llama Stack with watsonx.data Milvus represents a powerful approach to building intelligent, context-aware applications. This complete RAG pipeline enables:\n",
    "\n",
    "- Storage and indexing of domain-specific knowledge  \n",
    "- Retrieval of relevant info via semantic similarity  \n",
    "- Generation of contextual responses with LLMs  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
