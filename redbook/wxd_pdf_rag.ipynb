{"metadata": {"kernelspec": {"name": "python311", "display_name": "Python 3.11 with Spark", "language": "python3"}, "language_info": {"name": "python", "version": "3.11.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat_minor": 4, "nbformat": 4, "cells": [{"cell_type": "markdown", "source": "# Step 0\n\n## - Click on the menu to the right\n\n## - \"Insert Project Token\"\n\n## - Use the downward arrow icon on the top menu to move the cell down and begin running the notebook", "metadata": {}}, {"cell_type": "code", "source": "# @hidden_cell\n# The project token is an authorization token that is used to access project resources like data sources, connections, and used by platform APIs.\nfrom project_lib import Project\nproject = Project(spark.sparkContext, '3e7a8204-6f39-4d09-b1f6-f7e402a82619', 'p-2+3GbZbYq+sIgGoWrWV0tR5A==;6icFCfbGU47QDr3ndJ2Keg==:OWvAeqjvHqOhZl4xYYWyRl5EBwer8SSRhVWC17konuMOcNFvEeD8V+Ee9nKPS26Kt8XAB6t+jiQEipuH1qXhxQqTV+RnyqyKAw==')\npc = project.project_context\n\nfrom ibm_watson_studio_lib import access_project_or_space\nwslib = access_project_or_space({'token':'p-2+3GbZbYq+sIgGoWrWV0tR5A==;6icFCfbGU47QDr3ndJ2Keg==:OWvAeqjvHqOhZl4xYYWyRl5EBwer8SSRhVWC17konuMOcNFvEeD8V+Ee9nKPS26Kt8XAB6t+jiQEipuH1qXhxQqTV+RnyqyKAw=='})\nwslib.spark.provide_spark_context(spark.sparkContext)", "metadata": {"id": "cfa55044-e7e0-4e80-b65d-27433b7dfad2", "msg_id": "4b05e1ba-4568-43f2-bb46-587adc1ec350"}, "outputs": [], "execution_count": 1}, {"cell_type": "markdown", "source": "# Load Data into Milvus for RAG\n\n\n# 1. Set up the environment\n\n## Install libraries\n\nwe need to install the pymilvus package to the watsonx.ai Python environment. ", "metadata": {}}, {"cell_type": "code", "source": "!pip install grpcio==1.60.0 \n!pip install pymilvus", "metadata": {"id": "18d833d6-d74f-4692-a764-4fcf2ceefe17", "msg_id": "e58f09cf-706b-4905-bbe4-59d44a53b484"}, "outputs": [{"name": "stdout", "text": "Requirement already satisfied: grpcio==1.60.0 in ./python/lib/python/site-packages (1.60.0)\nRequirement already satisfied: pymilvus in ./python/lib/python/site-packages (2.5.0)\nRequirement already satisfied: setuptools>69 in ./python/lib/python/site-packages (from pymilvus) (72.1.0)\nRequirement already satisfied: grpcio<=1.67.1,>=1.49.1 in ./python/lib/python/site-packages (from pymilvus) (1.60.0)\nRequirement already satisfied: protobuf>=3.20.0 in ./python/lib/python/site-packages (from pymilvus) (4.21.12)\nRequirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in ./python/lib/python/site-packages (from pymilvus) (1.0.1)\nRequirement already satisfied: ujson>=2.0.0 in ./python/lib/python/site-packages (from pymilvus) (5.10.0)\nRequirement already satisfied: pandas>=1.2.4 in ./python/lib/python/site-packages (from pymilvus) (2.1.4)\nRequirement already satisfied: milvus-lite>=2.4.0 in ./python/lib/python/site-packages (from pymilvus) (2.4.10)\nRequirement already satisfied: tqdm in ./python/lib/python/site-packages (from milvus-lite>=2.4.0->pymilvus) (4.65.0)\nRequirement already satisfied: numpy<2,>=1.23.2 in ./python/lib/python/site-packages (from pandas>=1.2.4->pymilvus) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in ./python/lib/python/site-packages (from pandas>=1.2.4->pymilvus) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in ./python/lib/python/site-packages (from pandas>=1.2.4->pymilvus) (2024.1)\nRequirement already satisfied: tzdata>=2022.1 in ./python/lib/python/site-packages (from pandas>=1.2.4->pymilvus) (2023.3)\nRequirement already satisfied: six>=1.5 in ./python/lib/python/site-packages (from python-dateutil>=2.8.2->pandas>=1.2.4->pymilvus) (1.16.0)\n", "output_type": "stream"}], "execution_count": 2}, {"cell_type": "markdown", "source": "# !!RESTART THE KERNAL AFTER pymilvus install!! \n\nCertain dependencies need to be persisted. Restarting the kernal allows this to occur", "metadata": {}}, {"cell_type": "code", "source": "!pip install ipython-sql==0.4.1\n!pip install sqlalchemy==1.4.46\n!pip install sqlalchemy==1.4.46 \"pyhive[presto]\"\n!pip install python-dotenv\n!pip install sentence_transformers\n!pip install langchain-community\n!pip install PyMuPDF\n\n# clean up the libraries not required", "metadata": {"id": "0a8b9b47-22fe-4a70-8d1a-78b635a986a6", "msg_id": "ed02aa17-c355-4096-a52d-6400e868c53a"}, "outputs": [{"name": "stdout", "text": "Requirement already satisfied: ipython-sql==0.4.1 in ./python/lib/python/site-packages (0.4.1)\nRequirement already satisfied: prettytable<1 in ./python/lib/python/site-packages (from ipython-sql==0.4.1) (0.7.2)\nRequirement already satisfied: ipython>=1.0 in ./python/lib/python/site-packages (from ipython-sql==0.4.1) (8.20.0)\nRequirement already satisfied: sqlalchemy>=0.6.7 in ./python/lib/python/site-packages (from ipython-sql==0.4.1) (1.4.46)\nRequirement already satisfied: sqlparse in ./python/lib/python/site-packages (from ipython-sql==0.4.1) (0.5.2)\nRequirement already satisfied: six in ./python/lib/python/site-packages (from ipython-sql==0.4.1) (1.16.0)\nRequirement already satisfied: ipython-genutils>=0.1.0 in ./python/lib/python/site-packages (from ipython-sql==0.4.1) (0.2.0)\nRequirement already satisfied: decorator in ./python/lib/python/site-packages (from ipython>=1.0->ipython-sql==0.4.1) (5.1.1)\nRequirement already satisfied: jedi>=0.16 in ./python/lib/python/site-packages (from ipython>=1.0->ipython-sql==0.4.1) (0.19.1)\nRequirement already satisfied: matplotlib-inline in ./python/lib/python/site-packages (from ipython>=1.0->ipython-sql==0.4.1) (0.1.6)\nRequirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in ./python/lib/python/site-packages (from ipython>=1.0->ipython-sql==0.4.1) (3.0.43)\nRequirement already satisfied: pygments>=2.4.0 in ./python/lib/python/site-packages (from ipython>=1.0->ipython-sql==0.4.1) (2.15.1)\nRequirement already satisfied: stack-data in ./python/lib/python/site-packages (from ipython>=1.0->ipython-sql==0.4.1) (0.2.0)\nRequirement already satisfied: traitlets>=5 in ./python/lib/python/site-packages (from ipython>=1.0->ipython-sql==0.4.1) (5.7.1)\nRequirement already satisfied: pexpect>4.3 in ./python/lib/python/site-packages (from ipython>=1.0->ipython-sql==0.4.1) (4.8.0)\nRequirement already satisfied: greenlet!=0.4.17 in ./python/lib/python/site-packages (from sqlalchemy>=0.6.7->ipython-sql==0.4.1) (3.0.1)\nRequirement already satisfied: parso<0.9.0,>=0.8.3 in ./python/lib/python/site-packages (from jedi>=0.16->ipython>=1.0->ipython-sql==0.4.1) (0.8.3)\nRequirement already satisfied: ptyprocess>=0.5 in ./python/lib/python/site-packages (from pexpect>4.3->ipython>=1.0->ipython-sql==0.4.1) (0.7.0)\nRequirement already satisfied: wcwidth in ./python/lib/python/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=1.0->ipython-sql==0.4.1) (0.2.5)\nRequirement already satisfied: executing in ./python/lib/python/site-packages (from stack-data->ipython>=1.0->ipython-sql==0.4.1) (0.8.3)\nRequirement already satisfied: asttokens in ./python/lib/python/site-packages (from stack-data->ipython>=1.0->ipython-sql==0.4.1) (2.0.5)\nRequirement already satisfied: pure-eval in ./python/lib/python/site-packages (from stack-data->ipython>=1.0->ipython-sql==0.4.1) (0.2.2)\nRequirement already satisfied: sqlalchemy==1.4.46 in ./python/lib/python/site-packages (1.4.46)\nRequirement already satisfied: greenlet!=0.4.17 in ./python/lib/python/site-packages (from sqlalchemy==1.4.46) (3.0.1)\nRequirement already satisfied: sqlalchemy==1.4.46 in ./python/lib/python/site-packages (1.4.46)\nRequirement already satisfied: pyhive[presto] in ./python/lib/python/site-packages (0.7.0)\nRequirement already satisfied: greenlet!=0.4.17 in ./python/lib/python/site-packages (from sqlalchemy==1.4.46) (3.0.1)\nRequirement already satisfied: future in ./python/lib/python/site-packages (from pyhive[presto]) (0.18.3)\nRequirement already satisfied: python-dateutil in ./python/lib/python/site-packages (from pyhive[presto]) (2.8.2)\nRequirement already satisfied: requests>=1.0.0 in ./python/lib/python/site-packages (from pyhive[presto]) (2.32.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in ./python/lib/python/site-packages (from requests>=1.0.0->pyhive[presto]) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in ./python/lib/python/site-packages (from requests>=1.0.0->pyhive[presto]) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in ./python/lib/python/site-packages (from requests>=1.0.0->pyhive[presto]) (1.26.19)\nRequirement already satisfied: certifi>=2017.4.17 in ./python/lib/python/site-packages (from requests>=1.0.0->pyhive[presto]) (2023.7.22)\nRequirement already satisfied: six>=1.5 in ./python/lib/python/site-packages (from python-dateutil->pyhive[presto]) (1.16.0)\nRequirement already satisfied: python-dotenv in ./python/lib/python/site-packages (1.0.1)\nRequirement already satisfied: sentence_transformers in ./python/lib/python/site-packages (3.3.1)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in ./python/lib/python/site-packages (from sentence_transformers) (4.47.0)\nRequirement already satisfied: tqdm in ./python/lib/python/site-packages (from sentence_transformers) (4.65.0)\nRequirement already satisfied: torch>=1.11.0 in ./python/lib/python/site-packages (from sentence_transformers) (2.1.2)\nRequirement already satisfied: scikit-learn in ./python/lib/python/site-packages (from sentence_transformers) (1.3.0)\nRequirement already satisfied: scipy in ./python/lib/python/site-packages (from sentence_transformers) (1.11.4)\nRequirement already satisfied: huggingface-hub>=0.20.0 in ./python/lib/python/site-packages (from sentence_transformers) (0.27.0)\nRequirement already satisfied: Pillow in ./python/lib/python/site-packages (from sentence_transformers) (10.3.0)\nRequirement already satisfied: filelock in ./python/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in ./python/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2023.10.0)\nRequirement already satisfied: packaging>=20.9 in ./python/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (23.2)\nRequirement already satisfied: pyyaml>=5.1 in ./python/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.1)\nRequirement already satisfied: requests in ./python/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in ./python/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.12.2)\nRequirement already satisfied: sympy in ./python/lib/python/site-packages (from torch>=1.11.0->sentence_transformers) (1.12)\nRequirement already satisfied: networkx in ./python/lib/python/site-packages (from torch>=1.11.0->sentence_transformers) (2.8.4)\nRequirement already satisfied: jinja2 in ./python/lib/python/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\nRequirement already satisfied: numpy>=1.17 in ./python/lib/python/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in ./python/lib/python/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in ./python/lib/python/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in ./python/lib/python/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.4.5)\nRequirement already satisfied: joblib>=1.1.1 in ./python/lib/python/site-packages (from scikit-learn->sentence_transformers) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in ./python/lib/python/site-packages (from scikit-learn->sentence_transformers) (2.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in ./python/lib/python/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in ./python/lib/python/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in ./python/lib/python/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in ./python/lib/python/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (1.26.19)\nRequirement already satisfied: certifi>=2017.4.17 in ./python/lib/python/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2023.7.22)\nRequirement already satisfied: mpmath>=0.19 in ./python/lib/python/site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\nRequirement already satisfied: langchain-community in ./python/lib/python/site-packages (0.3.12)\nRequirement already satisfied: PyYAML>=5.3 in ./python/lib/python/site-packages (from langchain-community) (6.0.1)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in ./python/lib/python/site-packages (from langchain-community) (1.4.46)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./python/lib/python/site-packages (from langchain-community) (3.9.5)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in ./python/lib/python/site-packages (from langchain-community) (0.6.7)\nRequirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in ./python/lib/python/site-packages (from langchain-community) (0.4.0)\nRequirement already satisfied: langchain<0.4.0,>=0.3.12 in ./python/lib/python/site-packages (from langchain-community) (0.3.12)\nRequirement already satisfied: langchain-core<0.4.0,>=0.3.25 in ./python/lib/python/site-packages (from langchain-community) (0.3.25)\nRequirement already satisfied: langsmith<0.3,>=0.1.125 in ./python/lib/python/site-packages (from langchain-community) (0.2.3)\nRequirement already satisfied: numpy<2,>=1.22.4 in ./python/lib/python/site-packages (from langchain-community) (1.26.4)\nRequirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in ./python/lib/python/site-packages (from langchain-community) (2.7.0)\nRequirement already satisfied: requests<3,>=2 in ./python/lib/python/site-packages (from langchain-community) (2.32.2)\nRequirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in ./python/lib/python/site-packages (from langchain-community) (8.2.2)\nRequirement already satisfied: aiosignal>=1.1.2 in ./python/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.2.0)\nRequirement already satisfied: attrs>=17.3.0 in ./python/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in ./python/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in ./python/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in ./python/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.3)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./python/lib/python/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.23.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in ./python/lib/python/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\nRequirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in ./python/lib/python/site-packages (from langchain<0.4.0,>=0.3.12->langchain-community) (0.3.3)\nRequirement already satisfied: pydantic<3.0.0,>=2.7.4 in ./python/lib/python/site-packages (from langchain<0.4.0,>=0.3.12->langchain-community) (2.10.3)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in ./python/lib/python/site-packages (from langchain-core<0.4.0,>=0.3.25->langchain-community) (1.33)\nRequirement already satisfied: packaging<25,>=23.2 in ./python/lib/python/site-packages (from langchain-core<0.4.0,>=0.3.25->langchain-community) (23.2)\nRequirement already satisfied: typing-extensions>=4.7 in ./python/lib/python/site-packages (from langchain-core<0.4.0,>=0.3.25->langchain-community) (4.12.2)\nRequirement already satisfied: httpx<1,>=0.23.0 in ./python/lib/python/site-packages (from langsmith<0.3,>=0.1.125->langchain-community) (0.26.0)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in ./python/lib/python/site-packages (from langsmith<0.3,>=0.1.125->langchain-community) (3.10.12)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./python/lib/python/site-packages (from langsmith<0.3,>=0.1.125->langchain-community) (1.0.0)\nRequirement already satisfied: python-dotenv>=0.21.0 in ./python/lib/python/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in ./python/lib/python/site-packages (from requests<3,>=2->langchain-community) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in ./python/lib/python/site-packages (from requests<3,>=2->langchain-community) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in ./python/lib/python/site-packages (from requests<3,>=2->langchain-community) (1.26.19)\nRequirement already satisfied: certifi>=2017.4.17 in ./python/lib/python/site-packages (from requests<3,>=2->langchain-community) (2023.7.22)\nRequirement already satisfied: greenlet!=0.4.17 in ./python/lib/python/site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.1)\nRequirement already satisfied: anyio in ./python/lib/python/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (3.5.0)\nRequirement already satisfied: httpcore==1.* in ./python/lib/python/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (1.0.2)\nRequirement already satisfied: sniffio in ./python/lib/python/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (1.3.0)\nRequirement already satisfied: h11<0.15,>=0.13 in ./python/lib/python/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in ./python/lib/python/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.25->langchain-community) (2.1)\nRequirement already satisfied: annotated-types>=0.6.0 in ./python/lib/python/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.12->langchain-community) (0.6.0)\nRequirement already satisfied: pydantic-core==2.27.1 in ./python/lib/python/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.12->langchain-community) (2.27.1)\nRequirement already satisfied: mypy-extensions>=0.3.0 in ./python/lib/python/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\nRequirement already satisfied: PyMuPDF in ./python/lib/python/site-packages (1.25.1)\n", "output_type": "stream"}], "execution_count": 3}, {"cell_type": "markdown", "source": "# Step 1: Document Ingestion\n\n## Load the pdf version of the watsonx.data documentation\n\nLoad the pdf version as an asset in the project using Spark", "metadata": {}}, {"cell_type": "code", "source": "import requests\nfrom pyspark.sql import SparkSession\nimport os\n\nspark = SparkSession.builder \\\n    .appName(\"Download watsonx.data PDF documentation\") \\\n    .getOrCreate()\n\ndef download_pdf(url, local_path):\n    \"\"\"Download the PDF from a URL and save it locally\"\"\"\n    response = requests.get(url)\n    if response.status_code == 200:\n        with open(local_path, 'wb') as file:\n            file.write(response.content)\n        print(f\"PDF downloaded successfully to {local_path}\")\n    else:\n        print(f\"Failed to download PDF. Status code: {response.status_code}\")\n\npdf_url = \"https://www.ibm.com/support/pages/system/files/inline-files/IBM%20watsonx.data%20version%202.0.3.pdf\"  \nlocal_file_path = \"wxd_doc_pdf.pdf\"  \n\ndownload_pdf(pdf_url, local_file_path)\n\nspark.stop()\n", "metadata": {"msg_id": "978e99a2-57df-465c-ad8b-a6df6194849f"}, "outputs": [{"name": "stdout", "text": "PDF downloaded successfully to wxd_doc_pdf.pdf\n", "output_type": "stream"}], "execution_count": 4}, {"cell_type": "markdown", "source": "## Code to extract the text from the pdf for embeddings", "metadata": {}}, {"cell_type": "code", "source": "\n# from langchain_community.document_loaders import DirectoryLoader\n# from langchain_community.document_loaders import PyPDFLoader\n\nasset_li=wslib.assets.list_assets(\"data_asset\")\nprint(asset_li)\n\nwslib.download_file(\"wxd_doc_pdf\")\n\nimport fitz # PyMuPDF\n\ndoc = fitz.open(\"wxd_doc_pdf\")\n\npdf_text = \"\"\n\nfor page in doc:\n    pdf_text += page.get_text()\n\n# print(pdf_text)", "metadata": {"id": "a16b9680-5689-4203-82b9-db682c6e8bfb", "msg_id": "0cd62522-5f5e-452c-a766-abdcedec4685"}, "outputs": [{"name": "stdout", "text": "[{'name': 'wxd_doc_pdf', 'description': None, 'asset_id': '1377b784-7528-4933-ade1-096dafbf2d7a', 'asset_type': 'data_asset', 'tags': None}]\n", "output_type": "stream"}], "execution_count": 5}, {"cell_type": "markdown", "source": "# Step 2: Document Chunking\n\nTo manage the large texts, we can divide the data into manageble chunks by logical units such as paragraphs, sentences, or fixed token lengths", "metadata": {}}, {"cell_type": "markdown", "source": "### Option 1: Chunking by Paragraphs", "metadata": {}}, {"cell_type": "code", "source": "def chunk_by_paragraphs(text):\n    paragraphs = text.split(\"\\n\\n\") # Assuming paragraphs are separated by two newlines\n    return [p.strip() for p in paragraphs if p.strip()]\n\nchunks = chunk_by_paragraphs(pdf_text)\n\n# print(chunks[:1]) # Preview of the first 5 chunks", "metadata": {"id": "12022d7e-0129-4493-9099-78b4a25ebce9", "msg_id": "f3b4025e-8b94-4162-bd61-a660ed461c70"}, "outputs": [], "execution_count": 6}, {"cell_type": "markdown", "source": "### Option 2: Chunking by Sentences\n\nIf the document structure is more fluid and paragraphs are not clearly defined, you could break the text into sentences. You can use _nltk_ or a similar library for sentence tokennization", "metadata": {}}, {"cell_type": "code", "source": "# import nltk\n\n# nltk.download('punkt')\n\n# def chunk_by_sentence(text):\n#     sentences = nltk.sent_tokenize(text)\n#     return sentences\n\n# chunks = chunk_by_sentence(pdf_text)\n\n# print(chunks[:5]) # Preview of the first 5 chunks", "metadata": {"id": "a5e90117-3793-4727-b22b-2f3814df300b", "msg_id": "46f314d1-0ae3-4aae-bb37-27b88c780f7c"}, "outputs": [], "execution_count": 7}, {"cell_type": "markdown", "source": "### Option 3: Chunking by Token length\n\nFor more control over the chunk size, you can spilt the text into chunks of a fixed number of tokens", "metadata": {}}, {"cell_type": "code", "source": "def chunk_by_tokens(text, max_tokens=512):\n    tokens = text.split() # Tokenize the whitespace\n    chunks = [tokens[i:i + max_tokens] for i in range(0, len(tokens), max_tokens)]\n    return [' '.join(chunk) for chunk in chunks]\n\nchunks = chunk_by_tokens(pdf_text)\n\n# print(chunks[:1]) # Preview of the first 5 chunks", "metadata": {"id": "ff585cc7-db9c-4af2-96a8-0628b6eb16d6", "msg_id": "851c3e41-5f92-4c66-8e24-d3c18f57ffe5"}, "outputs": [], "execution_count": 8}, {"cell_type": "markdown", "source": "# Step 3: Embedding Generation", "metadata": {}}, {"cell_type": "code", "source": "wslib.list_connections", "metadata": {"id": "b9a699c2-e297-4b58-8c67-760f0d5340b3", "msg_id": "d256c025-bd31-4bcb-939a-9c322798aed1"}, "outputs": [{"execution_count": 9, "output_type": "execute_result", "data": {"text/plain": "<bound method Agent.list_connections of <ibm_watson_studio_lib.impl.agent.Agent object at 0x7f0452092cd0>>"}, "metadata": {}}], "execution_count": 9}, {"cell_type": "code", "source": "# note if you named your Milvus connection something other than \"Milvus Connection\" Please replace the name below\n\nmilvus_credentials = wslib.get_connection(\"custom-service\")\nprint(milvus_credentials['host'])\n# replace the milvus connection asset in the project", "metadata": {"id": "4967f991-b017-460e-a2df-916dce49b94f", "msg_id": "fd9eb7fe-6b20-4d6e-8060-7cd910a15913"}, "outputs": [{"name": "stdout", "text": "6c0c63ab-ecd7-45bd-bea8-c4d2d6fe976a.cie9nt2d0bngcm5pd3og.lakehouse.dev.appdomain.cloud\n", "output_type": "stream"}], "execution_count": 10}, {"cell_type": "code", "source": "#milvus_credentials", "metadata": {"id": "73ec2be5-caa9-4102-813d-072cd5186489", "msg_id": "54cdbe70-1f01-400f-8c25-77e53d73fb4a"}, "outputs": [], "execution_count": 11}, {"cell_type": "code", "source": "from pymilvus import(\n    Milvus,\n    IndexType,\n    Status,\n    connections,\n    FieldSchema,\n    DataType,\n    Collection,\n    CollectionSchema,\n)\n\n\nurl = milvus_credentials['host']\nport = milvus_credentials['port']\napikey = milvus_credentials['password']\napiuser = 'ibmlhapikey'\n\n\nconnections.connect(alias=\"default\", \n                    host=url, \n                    port=port, \n                    user=apiuser, \n                    password=apikey, \n                    secure=True)", "metadata": {"id": "f0aad8eb-08e6-49ef-b868-44d0f246525f", "msg_id": "9f2884d1-a321-4c47-90e3-f00d189fff4b"}, "outputs": [], "execution_count": 12}, {"cell_type": "code", "source": "# Create a new collection\ncollection_description = 'wxd docs pdf'\ncollection_name = 'wxd_documentation2'", "metadata": {"id": "d2dae158-da1b-433a-bc74-0ab4249cf879", "msg_id": "3984f5ec-34e9-419a-bff1-bcaed24c36fb"}, "outputs": [], "execution_count": 13}, {"cell_type": "code", "source": "# Create collection - define fields + schema\n\nfields = [\n    FieldSchema(name=\"document_id\", dtype=DataType.INT64), # Document Id\n    FieldSchema(name=\"chunk_id\",  dtype=DataType.VARCHAR, is_primary=True, max_length=20000), # Chunk Id\n    FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=384), # embedding dimension\n]\n\n# Create a schema\nschema = CollectionSchema(fields, collection_description)\n\n# Create a collection\ncollection = Collection(collection_name, schema)\n\n# Create index\nindex_params = {\n        'metric_type':'L2',\n        'index_type':\"IVF_FLAT\",\n        'params':{\"nlist\":2048}\n}\n\ncollection.create_index(field_name=\"embedding\", index_params=index_params)\n\n", "metadata": {"id": "9aa62853-7fb8-43c6-be9e-f6480ec4b265", "msg_id": "f63dc3d1-547a-4ae4-b1c3-0d4e01cdbc22"}, "outputs": [{"execution_count": 14, "output_type": "execute_result", "data": {"text/plain": "Status(code=0, message=)"}, "metadata": {}}], "execution_count": 14}, {"cell_type": "code", "source": "# we can run a check to see the collections in our milvus instance and we see the new collection has been created \n\nfrom pymilvus import utility\nutility.list_collections()", "metadata": {"id": "7af099cb-7bc0-4ae2-90fb-284b7d9ce12e", "msg_id": "60c7dcac-3254-47f4-9abd-f7c269372f12"}, "outputs": [{"execution_count": 15, "output_type": "execute_result", "data": {"text/plain": "['wxd_documentation1',\n 'wxd_documentation2',\n 'test_collection',\n 'wxd_documentation']"}, "metadata": {}}], "execution_count": 15}, {"cell_type": "code", "source": "# load data into Milvus\nimport pandas as pd\nfrom sentence_transformers import SentenceTransformer\nfrom pymilvus import Collection, connections\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndata = []\nprint(\"len of chunks\", len(chunks))\nmodel = SentenceTransformer('sentence-transformers/all-minilm-l12-v2') # 384 dim\n\n# for i in range(len(chunks)):\nfor i in range(10):\n    # Create vector embeddings + data\n    passage_embeddings = model.encode(chunks[i])\n    document_id = i\n    data.append({\"document_id\":document_id, \"chunk_id\":chunks[i],\"embedding\": passage_embeddings})\n    print(i)\nout = collection.insert(data)\n    \nprint(\"wxd chunk: \\'\" + chunks[i][0] + \"\\' has been loaded.\")", "metadata": {"id": "5c531cc4-56ec-4dcd-a61f-9cca82e963cb", "msg_id": "2eee2c61-7133-424a-9c15-5cbfc26d2e00"}, "outputs": [{"name": "stderr", "text": "2024-12-17 20:01:08.582472: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n", "output_type": "stream"}, {"name": "stdout", "text": "len of chunks 302\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nwxd chunk: 'q' has been loaded.\n", "output_type": "stream"}], "execution_count": 16}, {"cell_type": "code", "source": "## check to ensure entities have been loaded into the collection\n\nbasic_collection = Collection(collection_name) \n\nbasic_collection.num_entities\nbasic_collection.flush()", "metadata": {"id": "d630914b-ec2a-47b9-92e0-d766c70a9a34", "msg_id": "b14a6318-2555-4113-a41b-731e6127a8fe"}, "outputs": [], "execution_count": 17}, {"cell_type": "code", "source": "", "metadata": {"id": "62512ec5-d1dc-4744-8eb1-aa1a34d14131"}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "# Step 4: Searching with Milvus", "metadata": {}}, {"cell_type": "code", "source": "from sentence_transformers import SentenceTransformer\nfrom pymilvus import(\n    Milvus,\n    IndexType,\n    Status,\n    connections,\n    FieldSchema,\n    DataType,\n    Collection,\n    CollectionSchema,\n)\n\nurl = milvus_credentials['host']\nport = milvus_credentials['port']\napikey = milvus_credentials['password']\napiuser = 'ibmlhapikey'\n\n\nconnections.connect(alias=\"default\", \n                    host=url, \n                    port=port, \n                    user=apiuser, \n                    password=apikey, \n                    secure=True)\n\n\n# Load collection\n\nbasic_collection = Collection(collection_name)      \nbasic_collection.load()\n\n# Query function\ndef query_milvus(query, num_results):\n    \n    # Vectorize query\n    model = SentenceTransformer('sentence-transformers/all-minilm-l12-v2') # 384 dim\n    query_embeddings = model.encode([query])\n\n    # Search\n    search_params = {\n        \"metric_type\": \"L2\", \n        \"params\": {\"nprobe\": 5}\n    }\n    results = basic_collection.search(\n        data=query_embeddings, \n        anns_field=\"embedding\", \n        param=search_params,\n        limit=num_results,\n        expr=None, \n        output_fields=['document_id'],\n    )\n    return results", "metadata": {"id": "a91053c4-296e-4f62-b4c6-d171b0b1eaaf", "msg_id": "6d4c2d83-a4e3-45b5-a08b-68074b08cf11"}, "outputs": [], "execution_count": 18}, {"cell_type": "markdown", "source": "# Prompt with LLM", "metadata": {"id": "d152a3f3-b1e8-4ee0-a004-18b4170bbe8e"}}, {"cell_type": "code", "source": "## Consider some questions to ask regarding the topic you have chosen \n\n#question_text = \"OTHER QUESTION TEXT\"\n\nquestion_text = \"How to add a new catalog?\"", "metadata": {"id": "a91e98ee-61df-47ce-a8b0-a2c9061d52ac", "msg_id": "87312e9f-0d21-44ae-992a-17793a49300a"}, "outputs": [], "execution_count": 19}, {"cell_type": "code", "source": "# Query Milvus \n\nnum_results = 3\nresults = query_milvus(question_text, num_results)\n\nrelevant_chunks = []\nfor i in range(num_results):    \n    text = results[0][i].id\n    relevant_chunks.append(text)\n    \nprint(relevant_chunks)", "metadata": {"id": "33a836e6-1a1d-4f1e-80f5-5808912d146d", "msg_id": "c464ed25-f6d5-4535-940b-748f8753e2a9"}, "outputs": [{"name": "stdout", "text": "['the left pane, go to Catalogs > All catalogs to view the available catalogs. 3. Select the catalog to open the catalog details page. 4. Click the catalog name and go to the Access control tab. 5. Go to Add collaborators > Add user and select a user role (Admin, Editor, or Viewer). 6. Search and select one or more users from the list and click Add. The user addition is successful. 7. Go to the Assets tab of the catalog details page, click the asset name, and go to the Access tab of the asset. 8. Click Add members, search for the added user, and click Add. Changing the owner of the asset Procedure 1. Go to the Assets tab of the catalog details page, click the asset name to open the asset details. 2. Click on the edit icon beside Asset owner and select a new user from the list. 3. Click, Apply. The asset owner is changed. Configure IBM Knowledge Catalog Do the following steps to associate a user to the table asset in IKC and assign the ownership. Procedure 1. Login to the IKC Cloud Pack for Data instance by using admin credentials. 2. From the watsonx.data home page, go to Access control. 3. Go to the Integration tab and click Integrate service. 4. Enter the following details: Field Description Service Select IBM Knowledge Catalog. 418 IBM watsonx.data: Release V2.0.0 Field Description Bucket catalog Select the applicable bucket catalogs for IKC governance. WKC endpoint Enter the IKC endpoint URL. API key Enter the Zen API key. For more information, see Generating API keys for authentication. 5. Click Integrate. The IKC integration is successful. Supported data types for IBM Knowledge Catalog Integration IBM Knowledge Catalog Integration with watsonx.data supports the following data types for transformation and masking. watsonx.data on Red Hat OpenShift \u2022 Varchar \u2022 Bigint \u2022 Boolean \u2022 Date \u2022 Double \u2022 Integer \u2022 Smallint \u2022 Timestamp \u2022 Tinyint Chapter 8. Integrations 419 Chapter 9. Working with Spark You can use watsonx.data to seamlessly integrate with Spark engine to achieve the following use cases: watsonx.data on Red Hat OpenShift \u2022 Ingesting large volumes of data into watsonx.data tables. Note: You can also cleanse and transform data before ingestion. \u2022 Table maintenance operations to enhance performance. \u2022 Complex analytics workloads that are difficult to represent as queries. External Spark engines External Spark engines are engines that exist in a different environment from where watsonx.data is deployed. You can deploy them in the following environments. watsonx.data on Red Hat OpenShift \u2022 Spark instance on Cloud \u2022 Spark on Cloud Pak for Data \u2022 Spark on EMR Based on the environment where the Spark engine is deployed, select the respective section to connect to watsonx.data: Working with Spark on Cloud Integrate watsonx.data with Analytics Engine serverless instance (Spark on Cloud) and run your Spark workloads. watsonx.data on Red Hat OpenShift Before you begin \u2022 Install IBM watsonx.data and ensure that the instance is up and running. For more information, see Installing watsonx.data on Red Hat OpenShift. \u2022 Provision an IBM Analytics', 'catalog. c) Click Save and restart engine. 2. Associate a catalog with an engine in topology view. a) Hover over the catalog that you want to associate with an engine and click the Manage associations icon. Chapter 5. Configuring user interface (UI) components 273 b) In Manage associations window, select the engine with which you want to associate the catalog. c) Click Save and restart engine. Exploring the catalog objects To explore the objects in a catalog, use one of the following methods: watsonx.data Developer edition watsonx.data on Red Hat OpenShift Procedure 1. Explore the catalog objects in list view. a) Click the name of catalog that you want to explore. Catalog information window opens. b) Click Objects. 2. Explore the catalog objects in topology view. a) Click the catalog that you want to explore. Catalog information window opens. b) Click Objects. Dissociating a catalog from an engine To dissociate a catalog with an engine, use one of the following methods: watsonx.data Developer edition watsonx.data on Red Hat OpenShift Procedure 1. Dissociate a catalog from an engine in list view. a) Click the overflow menu icon and then click Manage associations. b) In Manage associations window, clear the checkbox in the Engine column. c) Click Save and restart engine. 2. Dissociate a catalog from an engine in topology view. a) Hover over the catalog that you want to dissociate from an engine and click the Manage associations icon. b) In Manage associations window, clear the checkbox in the Engine column. c) Click Save and restart engine. Deleting an engine To delete an engine, use one of the following methods: watsonx.data on Red Hat OpenShift Procedure 1. Delete an engine in list view. a) Click the overflow menu icon at the end of the row and click Delete. A delete confirmation dialog appears. b) Click Delete. 2. Deleting a database in topology view. 274 IBM watsonx.data: Release V2.0.0 a) Hover over the engine that you want to delete and click the Delete icon. A delete confirmation dialog appears. b) Click Delete. Managing the Spark engine details IBM watsonx.data allows you to view and edit the details of a Spark engine. You can also monitor the status of the applications that are submitted in the instance. watsonx.data on Red Hat OpenShift About this task Viewing Spark details You can view the Spark details in list and topology views. 1. Click the name of Spark engine (either from list or topology view). Engine information window opens. 2. In the Details tab, you can view the following details: Field Description Display name The Spark engine name. Engine ID The unique identifier of the Spark instance. Description The description of the engine. Tags The tag that is specified at the time of registering an engine. Type The engine type. Here, IBM Analytics Engine (Spark). Instance URL The IBM Analytics Engine (Spark) URL. watsonx.data application endpoint The application submission endpoint. To submit an application by using API, see API Docs. Instance API endpoint The IBM Analytics Engine (Spark) API endpoint. History server endpoint The IBM Analytics Engine (Spark)', 'name. 7. Select all the data present on the bucket to sync or create a catalog without any preexisting data. 8. After registration, click the catalog and go to Sync logs. You can see the status of synchronization (success, failure, partial success) and the last sync time. 9. After completing the synchronization, go to the Data manager. You can see the catalog that you created and the tables that are pulled from the bucket created by Snowflake. Note: You can use SQL editor (Query workspace) and use these tables to select query and insert data into existing table. 10. If Snowflake user makes data changes in the bucket and watsonx.data wants to pull those changes in the bucket, then click Sync data from UI for three strategy options: 402 IBM watsonx.data: Release V2.0.0 a. Sync all data: Synchronize all the data or update the existing table that was promoted earlier. b. Sync new data only: Pull the newly created table only. c. Sync existing data only: Update the table registered earlier to the latest changes only. Chapter 6. Working with data 403 Chapter 7. Connecting to a Presto server Presto CLI provides a terminal-based interactive shell to run queries. You can connect to Presto server either through Presto CLI installed as part of the ibm-lh-client package or through Presto CLI installed separately. watsonx.data on Red Hat OpenShift To connect to a Presto Server from a client program or CLI, the following items are required: \u2022 Hostname and port for the Presto server or workstation where the IBM watsonx.data developer or stand-alone is installed. \u2022 Certificates served by the Presto server to establish trust. \u2022 Authorized user credentials to access the Presto server. Note: It is important to connect to the Presto server that uses its hostname and not its IP address. Because the TLS certificate that is served by the Presto Server is associated with a fully qualified host and domain-name (FQDN). Client programs, typically, cannot establish trust by using the IP address. With OpenShift Ingress in particular, DNS entries, based on hostnames, play an important role in routing to the intended Kubernetes Service. Tip: To confirm there is network access from your client workstation that needs to connect a Presto server you can test the access by using one of the following commands: curl -ki https://<presto-hostname>:<presto-portnumber> nc -v <presto-hostname> <presto-portnumber> \u2022 The Presto server exposes a HTTPs (TCP) port. Therefore, you can use any convenient HTTP or TCP- based utility to ensure that the Presto server can be accessed from your network. \u2022 When you run the curl command, the server may return HTTP/1.1 401 Unauthorized response. This is expected as the server is secured by authentication. \u2022 When you run the nc command, the server returns a success message. In watsonx.data, you can connect to the Presto server in multiple ways based on the platform and utilities you are using. See the following sections for more details: \u2022 Using built-in presto-cli in the developer edition \u2022 Using presto-cli and presto-run in the ibm-lh-client package \u2022 Using presto-cli executable (remote) \u2013']\n", "output_type": "stream"}], "execution_count": 20}, {"cell_type": "code", "source": "def make_prompt(context, question_text):\n    return (f\"{context}\\n\\nPlease answer a question using this text. \"\n          + f\"If the question is unanswerable, say \\\"unanswerable\\\".\"\n          + f\"\\n\\nQuestion: {question_text}\")\n\n\n# Build prompt w/ Milvus results\n# Embed retrieved passages(context) and user question into into prompt text\n\ncontext = \"\\n\\n\".join(relevant_chunks)\nprompt = make_prompt(context, question_text)\n\nprint(prompt)", "metadata": {"id": "85a0ccc0-43ef-4976-be3b-87a99a34f9be", "msg_id": "29346ac7-261a-424e-b8ad-3c3f27840772"}, "outputs": [{"name": "stdout", "text": "the left pane, go to Catalogs > All catalogs to view the available catalogs. 3. Select the catalog to open the catalog details page. 4. Click the catalog name and go to the Access control tab. 5. Go to Add collaborators > Add user and select a user role (Admin, Editor, or Viewer). 6. Search and select one or more users from the list and click Add. The user addition is successful. 7. Go to the Assets tab of the catalog details page, click the asset name, and go to the Access tab of the asset. 8. Click Add members, search for the added user, and click Add. Changing the owner of the asset Procedure 1. Go to the Assets tab of the catalog details page, click the asset name to open the asset details. 2. Click on the edit icon beside Asset owner and select a new user from the list. 3. Click, Apply. The asset owner is changed. Configure IBM Knowledge Catalog Do the following steps to associate a user to the table asset in IKC and assign the ownership. Procedure 1. Login to the IKC Cloud Pack for Data instance by using admin credentials. 2. From the watsonx.data home page, go to Access control. 3. Go to the Integration tab and click Integrate service. 4. Enter the following details: Field Description Service Select IBM Knowledge Catalog. 418 IBM watsonx.data: Release V2.0.0 Field Description Bucket catalog Select the applicable bucket catalogs for IKC governance. WKC endpoint Enter the IKC endpoint URL. API key Enter the Zen API key. For more information, see Generating API keys for authentication. 5. Click Integrate. The IKC integration is successful. Supported data types for IBM Knowledge Catalog Integration IBM Knowledge Catalog Integration with watsonx.data supports the following data types for transformation and masking. watsonx.data on Red Hat OpenShift \u2022 Varchar \u2022 Bigint \u2022 Boolean \u2022 Date \u2022 Double \u2022 Integer \u2022 Smallint \u2022 Timestamp \u2022 Tinyint Chapter 8. Integrations 419 Chapter 9. Working with Spark You can use watsonx.data to seamlessly integrate with Spark engine to achieve the following use cases: watsonx.data on Red Hat OpenShift \u2022 Ingesting large volumes of data into watsonx.data tables. Note: You can also cleanse and transform data before ingestion. \u2022 Table maintenance operations to enhance performance. \u2022 Complex analytics workloads that are difficult to represent as queries. External Spark engines External Spark engines are engines that exist in a different environment from where watsonx.data is deployed. You can deploy them in the following environments. watsonx.data on Red Hat OpenShift \u2022 Spark instance on Cloud \u2022 Spark on Cloud Pak for Data \u2022 Spark on EMR Based on the environment where the Spark engine is deployed, select the respective section to connect to watsonx.data: Working with Spark on Cloud Integrate watsonx.data with Analytics Engine serverless instance (Spark on Cloud) and run your Spark workloads. watsonx.data on Red Hat OpenShift Before you begin \u2022 Install IBM watsonx.data and ensure that the instance is up and running. For more information, see Installing watsonx.data on Red Hat OpenShift. \u2022 Provision an IBM Analytics\n\ncatalog. c) Click Save and restart engine. 2. Associate a catalog with an engine in topology view. a) Hover over the catalog that you want to associate with an engine and click the Manage associations icon. Chapter 5. Configuring user interface (UI) components 273 b) In Manage associations window, select the engine with which you want to associate the catalog. c) Click Save and restart engine. Exploring the catalog objects To explore the objects in a catalog, use one of the following methods: watsonx.data Developer edition watsonx.data on Red Hat OpenShift Procedure 1. Explore the catalog objects in list view. a) Click the name of catalog that you want to explore. Catalog information window opens. b) Click Objects. 2. Explore the catalog objects in topology view. a) Click the catalog that you want to explore. Catalog information window opens. b) Click Objects. Dissociating a catalog from an engine To dissociate a catalog with an engine, use one of the following methods: watsonx.data Developer edition watsonx.data on Red Hat OpenShift Procedure 1. Dissociate a catalog from an engine in list view. a) Click the overflow menu icon and then click Manage associations. b) In Manage associations window, clear the checkbox in the Engine column. c) Click Save and restart engine. 2. Dissociate a catalog from an engine in topology view. a) Hover over the catalog that you want to dissociate from an engine and click the Manage associations icon. b) In Manage associations window, clear the checkbox in the Engine column. c) Click Save and restart engine. Deleting an engine To delete an engine, use one of the following methods: watsonx.data on Red Hat OpenShift Procedure 1. Delete an engine in list view. a) Click the overflow menu icon at the end of the row and click Delete. A delete confirmation dialog appears. b) Click Delete. 2. Deleting a database in topology view. 274 IBM watsonx.data: Release V2.0.0 a) Hover over the engine that you want to delete and click the Delete icon. A delete confirmation dialog appears. b) Click Delete. Managing the Spark engine details IBM watsonx.data allows you to view and edit the details of a Spark engine. You can also monitor the status of the applications that are submitted in the instance. watsonx.data on Red Hat OpenShift About this task Viewing Spark details You can view the Spark details in list and topology views. 1. Click the name of Spark engine (either from list or topology view). Engine information window opens. 2. In the Details tab, you can view the following details: Field Description Display name The Spark engine name. Engine ID The unique identifier of the Spark instance. Description The description of the engine. Tags The tag that is specified at the time of registering an engine. Type The engine type. Here, IBM Analytics Engine (Spark). Instance URL The IBM Analytics Engine (Spark) URL. watsonx.data application endpoint The application submission endpoint. To submit an application by using API, see API Docs. Instance API endpoint The IBM Analytics Engine (Spark) API endpoint. History server endpoint The IBM Analytics Engine (Spark)\n\nname. 7. Select all the data present on the bucket to sync or create a catalog without any preexisting data. 8. After registration, click the catalog and go to Sync logs. You can see the status of synchronization (success, failure, partial success) and the last sync time. 9. After completing the synchronization, go to the Data manager. You can see the catalog that you created and the tables that are pulled from the bucket created by Snowflake. Note: You can use SQL editor (Query workspace) and use these tables to select query and insert data into existing table. 10. If Snowflake user makes data changes in the bucket and watsonx.data wants to pull those changes in the bucket, then click Sync data from UI for three strategy options: 402 IBM watsonx.data: Release V2.0.0 a. Sync all data: Synchronize all the data or update the existing table that was promoted earlier. b. Sync new data only: Pull the newly created table only. c. Sync existing data only: Update the table registered earlier to the latest changes only. Chapter 6. Working with data 403 Chapter 7. Connecting to a Presto server Presto CLI provides a terminal-based interactive shell to run queries. You can connect to Presto server either through Presto CLI installed as part of the ibm-lh-client package or through Presto CLI installed separately. watsonx.data on Red Hat OpenShift To connect to a Presto Server from a client program or CLI, the following items are required: \u2022 Hostname and port for the Presto server or workstation where the IBM watsonx.data developer or stand-alone is installed. \u2022 Certificates served by the Presto server to establish trust. \u2022 Authorized user credentials to access the Presto server. Note: It is important to connect to the Presto server that uses its hostname and not its IP address. Because the TLS certificate that is served by the Presto Server is associated with a fully qualified host and domain-name (FQDN). Client programs, typically, cannot establish trust by using the IP address. With OpenShift Ingress in particular, DNS entries, based on hostnames, play an important role in routing to the intended Kubernetes Service. Tip: To confirm there is network access from your client workstation that needs to connect a Presto server you can test the access by using one of the following commands: curl -ki https://<presto-hostname>:<presto-portnumber> nc -v <presto-hostname> <presto-portnumber> \u2022 The Presto server exposes a HTTPs (TCP) port. Therefore, you can use any convenient HTTP or TCP- based utility to ensure that the Presto server can be accessed from your network. \u2022 When you run the curl command, the server may return HTTP/1.1 401 Unauthorized response. This is expected as the server is secured by authentication. \u2022 When you run the nc command, the server returns a success message. In watsonx.data, you can connect to the Presto server in multiple ways based on the platform and utilities you are using. See the following sections for more details: \u2022 Using built-in presto-cli in the developer edition \u2022 Using presto-cli and presto-run in the ibm-lh-client package \u2022 Using presto-cli executable (remote) \u2013\n\nPlease answer a question using this text. If the question is unanswerable, say \"unanswerable\".\n\nQuestion: How to add a new catalog?\n", "output_type": "stream"}], "execution_count": 21}, {"cell_type": "code", "source": "from ibm_watson_machine_learning.foundation_models import Model\nfrom ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n\n# Model Parameters\nparams = {\n        GenParams.DECODING_METHOD: \"greedy\",\n        GenParams.MIN_NEW_TOKENS: 1,\n        GenParams.MAX_NEW_TOKENS: 500,\n        GenParams.TEMPERATURE: 0,\n}\n\n\n# please note if using a cloud account in a different geography the cloud URL will be different \n# Refer to this list: \n#    Dallas - https://us-south.ml.cloud.ibm.com\n#    London - https://eu-gb.ml.cloud.ibm.com\n#    Frankfurt - https://eu-de.ml.cloud.ibm.com\n#    Tokyo - https://jp-tok.ml.cloud.ibm.com\ncreds = {\n    \"url\": 'https://us-south.ml.cloud.ibm.com',\n    \"apikey\": 'B0OGWJN6LHso_An_cJa6pGr5HxQf74KHkXxEH3PtFr4U',\n}\nmodel = Model(\n        model_id='ibm/granite-13b-chat-v2', \n        params=params, \n        credentials=creds, \n        project_id=wslib.here.get_ID()\n)\n\n# Prompt LLM\nresponse = model.generate_text(prompt)\nprint(f\"Question: {question_text}{response}\")", "metadata": {"id": "924b3368-a649-48fb-a90c-81eee6f81ed6", "msg_id": "5b04e445-4891-4ea9-a8b9-69bb29d5abb9"}, "outputs": [{"name": "stderr", "text": "[W 2024-12-17 20:20:57,355.355 ibm_watson_machine_learning.wml_client_error] Cannot set Project or Space\nReason: {\"id\":\"WSCPA0000E\",\"code\":404,\"error\":\"Not Found\",\"reason\":\"Failed to retrieve project: 3e7a8204-6f39-4d09-b1f6-f7e402a82619. Failed to retrieve Cloudant document: 3e7a8204-6f39-4d09-b1f6-f7e402a82619. not_found.\",\"message\":\"Resource requested by the client was not found.\"}\n", "output_type": "stream"}, {"traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mCannotSetProjectOrSpace\u001b[0m                   Traceback (most recent call last)", "Cell \u001b[0;32mIn[26], line 23\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# please note if using a cloud account in a different geography the cloud URL will be different \u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Refer to this list: \u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#    Dallas - https://us-south.ml.cloud.ibm.com\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#    London - https://eu-gb.ml.cloud.ibm.com\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#    Frankfurt - https://eu-de.ml.cloud.ibm.com\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#    Tokyo - https://jp-tok.ml.cloud.ibm.com\u001b[39;00m\n\u001b[1;32m     19\u001b[0m creds \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://us-south.ml.cloud.ibm.com\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapikey\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB0OGWJN6LHso_An_cJa6pGr5HxQf74KHkXxEH3PtFr4U\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     22\u001b[0m }\n\u001b[0;32m---> 23\u001b[0m model \u001b[38;5;241m=\u001b[39m Model(\n\u001b[1;32m     24\u001b[0m         model_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mibm/granite-13b-chat-v2\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     25\u001b[0m         params\u001b[38;5;241m=\u001b[39mparams, \n\u001b[1;32m     26\u001b[0m         credentials\u001b[38;5;241m=\u001b[39mcreds, \n\u001b[1;32m     27\u001b[0m         project_id\u001b[38;5;241m=\u001b[39mwslib\u001b[38;5;241m.\u001b[39mhere\u001b[38;5;241m.\u001b[39mget_ID()\n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Prompt LLM\u001b[39;00m\n\u001b[1;32m     31\u001b[0m response \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate_text(prompt)\n", "File \u001b[0;32m~/python/lib/python/site-packages/ibm_watson_machine_learning/foundation_models/model.py:82\u001b[0m, in \u001b[0;36mModel.__init__\u001b[0;34m(self, model_id, credentials, params, project_id, space_id, verify)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     76\u001b[0m              model_id: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m     77\u001b[0m              credentials: \u001b[38;5;28mdict\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     80\u001b[0m              space_id: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     81\u001b[0m              verify\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 82\u001b[0m     ModelInference\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     83\u001b[0m                             model_id\u001b[38;5;241m=\u001b[39mmodel_id,\n\u001b[1;32m     84\u001b[0m                             credentials\u001b[38;5;241m=\u001b[39mcredentials,\n\u001b[1;32m     85\u001b[0m                             params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m     86\u001b[0m                             project_id\u001b[38;5;241m=\u001b[39mproject_id,\n\u001b[1;32m     87\u001b[0m                             space_id\u001b[38;5;241m=\u001b[39mspace_id,\n\u001b[1;32m     88\u001b[0m                             verify\u001b[38;5;241m=\u001b[39mverify\n\u001b[1;32m     89\u001b[0m                             )\n", "File \u001b[0;32m~/python/lib/python/site-packages/ibm_watson_machine_learning/foundation_models/inference/model_inference.py:140\u001b[0m, in \u001b[0;36mModelInference.__init__\u001b[0;34m(self, model_id, deployment_id, params, credentials, project_id, space_id, verify, api_client)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mset\u001b[38;5;241m.\u001b[39mdefault_space(space_id)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m project_id:\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mset\u001b[38;5;241m.\u001b[39mdefault_project(project_id)\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m api_client:\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidMultipleArguments(params_names_list\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspace_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproject_id\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    143\u001b[0m                                    reason\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of the arguments were provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n", "File \u001b[0;32m~/python/lib/python/site-packages/ibm_watson_machine_learning/Set.py:109\u001b[0m, in \u001b[0;36mSet.default_project\u001b[0;34m(self, project_id)\u001b[0m\n\u001b[1;32m    107\u001b[0m project_details \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(project_endpoint, headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39m_get_headers())\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m project_details\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m project_details\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m204\u001b[39m:\n\u001b[0;32m--> 109\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSetProjectOrSpace(reason\u001b[38;5;241m=\u001b[39mproject_details\u001b[38;5;241m.\u001b[39mtext)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mproject_type \u001b[38;5;241m=\u001b[39m project_details\u001b[38;5;241m.\u001b[39mjson()[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentity\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstorage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m]\n", "\u001b[0;31mCannotSetProjectOrSpace\u001b[0m: Cannot set Project or Space\nReason: {\"id\":\"WSCPA0000E\",\"code\":404,\"error\":\"Not Found\",\"reason\":\"Failed to retrieve project: 3e7a8204-6f39-4d09-b1f6-f7e402a82619. Failed to retrieve Cloudant document: 3e7a8204-6f39-4d09-b1f6-f7e402a82619. not_found.\",\"message\":\"Resource requested by the client was not found.\"}"], "ename": "CannotSetProjectOrSpace", "evalue": "Cannot set Project or Space\nReason: {\"id\":\"WSCPA0000E\",\"code\":404,\"error\":\"Not Found\",\"reason\":\"Failed to retrieve project: 3e7a8204-6f39-4d09-b1f6-f7e402a82619. Failed to retrieve Cloudant document: 3e7a8204-6f39-4d09-b1f6-f7e402a82619. not_found.\",\"message\":\"Resource requested by the client was not found.\"}", "output_type": "error"}], "execution_count": 26}, {"cell_type": "code", "source": "", "metadata": {}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "", "metadata": {}, "outputs": [], "execution_count": null}]}